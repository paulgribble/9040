[
  {
    "objectID": "uv.html",
    "href": "uv.html",
    "title": "uv",
    "section": "",
    "text": "When you write Python code, you will almost always rely on packages—code that other people have written and shared, so you don’t have to reinvent the wheel. For example, NumPy gives you fast array math, SciPy gives you signal processing and statistics, and Matplotlib gives you plotting.\nBut different projects may need different packages, or even different versions of the same package. If you just install everything globally on your computer, things can conflict and break in confusing ways. Imagine you have one project that needs version 1.x of a package and another that needs version 2.x—they can’t both be installed at the same time in the same place.\nThe solution is to give each project its own isolated environment with its own set of packages. This is what uv helps you do."
  },
  {
    "objectID": "uv.html#the-problem-uv-solves",
    "href": "uv.html#the-problem-uv-solves",
    "title": "uv",
    "section": "",
    "text": "When you write Python code, you will almost always rely on packages—code that other people have written and shared, so you don’t have to reinvent the wheel. For example, NumPy gives you fast array math, SciPy gives you signal processing and statistics, and Matplotlib gives you plotting.\nBut different projects may need different packages, or even different versions of the same package. If you just install everything globally on your computer, things can conflict and break in confusing ways. Imagine you have one project that needs version 1.x of a package and another that needs version 2.x—they can’t both be installed at the same time in the same place.\nThe solution is to give each project its own isolated environment with its own set of packages. This is what uv helps you do."
  },
  {
    "objectID": "uv.html#what-is-uv",
    "href": "uv.html#what-is-uv",
    "title": "uv",
    "section": "What is uv?",
    "text": "What is uv?\nuv is a Python package and project manager made by a company called Astral. It handles three things for you:\n\nInstalling Python itself — you can use uv to install and manage different Python versions.\nCreating isolated project environments — each project gets its own private set of packages that won’t interfere with anything else on your computer.\nInstalling and managing packages — adding, removing, and updating the packages your project depends on.\n\nYou may have heard of other tools like pip, virtualenv, or conda that do similar things. uv is a newer, faster, and simpler alternative that combines several of these tools into one. It is the tool we will use throughout this course.\n\n\n\n\n\n\nNote\n\n\n\nYou should already have uv installed from the Setup page. If not, go back and follow those instructions first."
  },
  {
    "objectID": "uv.html#the-basic-workflow",
    "href": "uv.html#the-basic-workflow",
    "title": "uv",
    "section": "The Basic Workflow",
    "text": "The Basic Workflow\nHere is the core workflow you will use over and over in this course. Every time you start a new assignment or project, you will follow roughly these steps:\n\n1. Initialize a new project\nOpen your terminal, navigate to wherever you want to work (e.g. your Psych_9040 folder), and run:\nmkdir my_project\ncd my_project\nuv init --python 3.12\nThe uv init command sets up a new project. The --python 3.12 flag tells uv which version of Python to use. After running this, you will see several new files in the folder:\nmy_project/\n├── .gitignore\n├── .python-version\n├── main.py\n├── pyproject.toml\n└── README.md\nDon’t worry about all of these right now. The two important ones are:\n\npyproject.toml — this is the “recipe card” for your project. It lists your project’s name, version, and most importantly, what packages it depends on.\nmain.py — a starter Python file with a simple “Hello world” program in it.\n\n\n\n2. Add packages\nLet’s say your project needs NumPy (for numerical computing) and Matplotlib (for plotting). You add them with:\nuv add numpy matplotlib\nuv will download and install these packages (and anything they depend on) into a private environment just for this project. It will also update your pyproject.toml to record that your project depends on these packages.\nIf you look at pyproject.toml afterwards, you’ll see something like:\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\ndependencies = [\n    \"matplotlib\",\n    \"numpy\",\n]\n\n\n3. Write your Python code\nOpen main.py in VS Code (or create a new .py file), and write your code. For example:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 2 * np.pi, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"A Sine Wave\")\nplt.xlabel(\"x\")\nplt.ylabel(\"sin(x)\")\nplt.savefig(\"sine_wave.png\")\nprint(\"Plot saved to sine_wave.png\")\n\n\n4. Run your code with uv run\nInstead of typing python main.py, you use:\nuv run python main.py\nWhy the extra uv run in front? Because uv run makes sure your code runs inside the project’s isolated environment, with all the correct packages available. If you just typed python main.py, your system might use a different Python that doesn’t have NumPy or Matplotlib installed, and you’d get an error.\n\n\n\n\n\n\nTip\n\n\n\nEvery time you run a Python script for this course, use uv run python your_script.py. It will become second nature quickly.\n\n\nSame thing for running ipython: launch it from the terminal using uv run ipython."
  },
  {
    "objectID": "uv.html#quick-reference",
    "href": "uv.html#quick-reference",
    "title": "uv",
    "section": "Quick Reference",
    "text": "Quick Reference\nHere is a summary of the uv commands you will use most often:\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nuv init --python 3.12\nInitialize a new project in the current folder\n\n\nuv add numpy\nAdd a package (e.g. numpy) to your project\n\n\nuv add numpy scipy matplotlib\nAdd multiple packages at once\n\n\nuv remove numpy\nRemove a package from your project\n\n\nuv run python my_script.py\nRun a Python script inside the project environment"
  },
  {
    "objectID": "uv.html#what-are-all-these-files",
    "href": "uv.html#what-are-all-these-files",
    "title": "uv",
    "section": "What are all these files?",
    "text": "What are all these files?\nAfter you have initialized a project and run some code, your project folder will contain several files and folders that uv manages. Here is what each one is for:\n\n\n\n\n\n\n\nFile / Folder\nWhat it is\n\n\n\n\npyproject.toml\nYour project’s configuration and list of dependencies. You can edit this by hand or use uv add / uv remove.\n\n\n.python-version\nA tiny file that records which Python version the project uses.\n\n\nuv.lock\nA detailed “snapshot” of every package and exact version installed. You don’t need to edit this—uv manages it automatically.\n\n\n.venv/\nThe virtual environment folder. This is where all the packages actually live on disk. You never need to look inside it.\n\n\nmain.py\nThe starter Python file that uv init creates. You can rename, edit, or delete it.\n\n\n.gitignore\nTells Git which files to ignore (like .venv/). More on this when we learn about Git.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should never manually edit uv.lock or anything inside .venv/. Let uv manage those for you."
  },
  {
    "objectID": "uv.html#a-common-beginner-mistake",
    "href": "uv.html#a-common-beginner-mistake",
    "title": "uv",
    "section": "A Common Beginner Mistake",
    "text": "A Common Beginner Mistake\nA very common mistake is to run your script like this:\npython my_script.py\nand get an error like:\nModuleNotFoundError: No module named 'numpy'\nThis happens because the plain python command is using your system Python, which doesn’t know about the packages you installed with uv add. The fix is always to use:\nuv run python my_script.py"
  },
  {
    "objectID": "uv.html#a-worked-example",
    "href": "uv.html#a-worked-example",
    "title": "uv",
    "section": "A Worked Example",
    "text": "A Worked Example\nHere is the full sequence of commands for starting a brand new assignment. Let’s say the assignment is called assignment_01:\n# Navigate into your course folder\ncd Psych_9040\n\n# Create and enter the assignment folder\nmkdir assignment_01\ncd assignment_01\n\n# Initialize the project\nuv init --python 3.12\n\n# Add any packages you need\nuv add numpy\n\n# Open the folder in VS Code\ncode .\nThen in VS Code, edit main.py (or create a new file), write your code, save it, and go back to the terminal to run it:\nuv run python main.py\nThat’s it. This is the workflow you will repeat throughout the course."
  },
  {
    "objectID": "uv.html#further-reading",
    "href": "uv.html#further-reading",
    "title": "uv",
    "section": "Further Reading",
    "text": "Further Reading\n\nuv: Working on Projects — the official guide\nuv: Project Concepts — more detailed explanations"
  },
  {
    "objectID": "plaintext-authoring.html",
    "href": "plaintext-authoring.html",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "What does a thesis or a paper consist of, structurally?\n\nSections and subsections with headings\nBody text (paragraphs of prose)\nEquations (inline and display)\nFigures with captions and cross-references\nTables\nCitations and a bibliography\nMaybe an abstract, appendices, acknowledgements\n\nHow much of that is formatting, and how much is content?\nA scientific document is structured text and data. The formatting, like fonts, margins, and line spacing, is something a template or style can handle. It’s a nuisance and a time-waster to fiddle with that stuff by hand. The principle of separation of content and style is one of the main rationales of plaintext authoring.\n\n\n\nIn Microsoft Word or Google Docs, content and formatting are entangled. You write a heading by selecting text and making it bold and 14pt. You don’t declare it as a heading in any structural sense (unless you use Styles, which almost nobody does consistently). The file format (.docx) is a zipped bundle of XML that is not human-readable. If Word (or Google) changes how it handles that format, or if you don’t have Word, you may not be able to open your own file in 10 years. If something goes wrong with the document you may not be able to “see” what changed because formatting and other things are hidden within menus and preferences settings.\n\n\n\nIn a plaintext workflow, you write your content in a simple text file, which is something you can open in Notepad, TextEdit, VS Code, or any other text editor on any computer, today or in 50 years. You mark up structure with lightweight, readable syntax (e.g. Markdown or LaTeX). Then a separate tool converts that source file into a PDF, HTML page, or even a Word document.\nIn the plaintext authoring model your document is:\n\nportable\narchivable\nversion-controllable\nplatform-independent\n\n\n\n\nHere’s the situation:\n\nMany journals accept LaTeX submissions directly. In physics, math, CS, and economics, LaTeX is the expected format. In neuroscience, biology, and psychology, it varies, but many journals do accept LaTeX (e.g., Nature, Nature Neuroscience, Science, PNAS, Cell, Neuron, eLife, J Neurosci, J Cogn Neurosci, all the APA journals, SAGE journals, Elsevier journals, Springer journals).\nPandoc can convert your Markdown or LaTeX source to .docx in one command. You write in plaintext, and when a journal demands Word, you generate a Word file at the end.\nThe real question is: where do you want to spend your time? Formatting in Word, or writing? Chasing down a reference that didn’t update, or trusting BibTeX? Emailing thesis_v3_FINAL_revised_FINAL2.docx back and forth, or using Git?\nYour thesis is long. Word can struggle with documents over ~50 pages that contain many figures and cross-references.\nGoogle Docs solves the collaboration and platform-independence problems nicely, and for short, simple documents it’s fine. But it shares Word’s limitation of entangling content and style, it has very limited equation support, it handles citations through third-party add-ons (Zotero, Paperpile), and try reformatting a 200-page Google Doc to a different journal’s style. You’ll also notice that Google Docs has no real support for cross-references, numbered equations, or figure captions that update automatically.\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nWord / Google Docs\nPlaintext (Markdown / LaTeX)\n\n\n\n\nHuman-readable source\nNo (binary/XML)\nYes\n\n\nVersion control (Git)\nDifficult\nNatural\n\n\nSeparation of content & style\nWeak (manual)\nStrong (by design)\n\n\nEquation support\nCumbersome\nExcellent (LaTeX math)\n\n\nCitation management\nPlugin-dependent\nBuilt-in (BibTeX/CSL)\n\n\nLong document stability\nDegrades\nRobust\n\n\nCollaboration\nGood (Google Docs)\nGit + plain text diffs / Overleaf\n\n\nOutput flexibility\nOne format\nPDF, HTML, Word, slides, …\n\n\nArchival / portability\nFormat-dependent\nPlain text lasts forever\n\n\nTypographic quality\nAcceptable\nSuperior (LaTeX)"
  },
  {
    "objectID": "plaintext-authoring.html#scientific-documents",
    "href": "plaintext-authoring.html#scientific-documents",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "What does a thesis or a paper consist of, structurally?\n\nSections and subsections with headings\nBody text (paragraphs of prose)\nEquations (inline and display)\nFigures with captions and cross-references\nTables\nCitations and a bibliography\nMaybe an abstract, appendices, acknowledgements\n\nHow much of that is formatting, and how much is content?\nA scientific document is structured text and data. The formatting, like fonts, margins, and line spacing, is something a template or style can handle. It’s a nuisance and a time-waster to fiddle with that stuff by hand. The principle of separation of content and style is one of the main rationales of plaintext authoring."
  },
  {
    "objectID": "plaintext-authoring.html#word-processors",
    "href": "plaintext-authoring.html#word-processors",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "In Microsoft Word or Google Docs, content and formatting are entangled. You write a heading by selecting text and making it bold and 14pt. You don’t declare it as a heading in any structural sense (unless you use Styles, which almost nobody does consistently). The file format (.docx) is a zipped bundle of XML that is not human-readable. If Word (or Google) changes how it handles that format, or if you don’t have Word, you may not be able to open your own file in 10 years. If something goes wrong with the document you may not be able to “see” what changed because formatting and other things are hidden within menus and preferences settings."
  },
  {
    "objectID": "plaintext-authoring.html#plaintext",
    "href": "plaintext-authoring.html#plaintext",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "In a plaintext workflow, you write your content in a simple text file, which is something you can open in Notepad, TextEdit, VS Code, or any other text editor on any computer, today or in 50 years. You mark up structure with lightweight, readable syntax (e.g. Markdown or LaTeX). Then a separate tool converts that source file into a PDF, HTML page, or even a Word document.\nIn the plaintext authoring model your document is:\n\nportable\narchivable\nversion-controllable\nplatform-independent"
  },
  {
    "objectID": "plaintext-authoring.html#journals",
    "href": "plaintext-authoring.html#journals",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "Here’s the situation:\n\nMany journals accept LaTeX submissions directly. In physics, math, CS, and economics, LaTeX is the expected format. In neuroscience, biology, and psychology, it varies, but many journals do accept LaTeX (e.g., Nature, Nature Neuroscience, Science, PNAS, Cell, Neuron, eLife, J Neurosci, J Cogn Neurosci, all the APA journals, SAGE journals, Elsevier journals, Springer journals).\nPandoc can convert your Markdown or LaTeX source to .docx in one command. You write in plaintext, and when a journal demands Word, you generate a Word file at the end.\nThe real question is: where do you want to spend your time? Formatting in Word, or writing? Chasing down a reference that didn’t update, or trusting BibTeX? Emailing thesis_v3_FINAL_revised_FINAL2.docx back and forth, or using Git?\nYour thesis is long. Word can struggle with documents over ~50 pages that contain many figures and cross-references.\nGoogle Docs solves the collaboration and platform-independence problems nicely, and for short, simple documents it’s fine. But it shares Word’s limitation of entangling content and style, it has very limited equation support, it handles citations through third-party add-ons (Zotero, Paperpile), and try reformatting a 200-page Google Doc to a different journal’s style. You’ll also notice that Google Docs has no real support for cross-references, numbered equations, or figure captions that update automatically."
  },
  {
    "objectID": "plaintext-authoring.html#summary",
    "href": "plaintext-authoring.html#summary",
    "title": "Plaintext Authoring",
    "section": "",
    "text": "Feature\nWord / Google Docs\nPlaintext (Markdown / LaTeX)\n\n\n\n\nHuman-readable source\nNo (binary/XML)\nYes\n\n\nVersion control (Git)\nDifficult\nNatural\n\n\nSeparation of content & style\nWeak (manual)\nStrong (by design)\n\n\nEquation support\nCumbersome\nExcellent (LaTeX math)\n\n\nCitation management\nPlugin-dependent\nBuilt-in (BibTeX/CSL)\n\n\nLong document stability\nDegrades\nRobust\n\n\nCollaboration\nGood (Google Docs)\nGit + plain text diffs / Overleaf\n\n\nOutput flexibility\nOne format\nPDF, HTML, Word, slides, …\n\n\nArchival / portability\nFormat-dependent\nPlain text lasts forever\n\n\nTypographic quality\nAcceptable\nSuperior (LaTeX)"
  },
  {
    "objectID": "plaintext-authoring.html#syntax",
    "href": "plaintext-authoring.html#syntax",
    "title": "Plaintext Authoring",
    "section": "2.1 Syntax",
    "text": "2.1 Syntax\nHere is what a Markdown document looks like:\n# My Research Paper\n\n## Introduction\n\nThis is a paragraph of text. You can make words **bold** or *italic*.\nYou can also include `inline code` for variable names or filenames.\n\nHere is a list of things:\n\n- First item\n- Second item\n- Third item\n\nAnd a numbered list:\n\n1. Do the experiment\n2. Analyze the data\n3. Write it up\n\n## Methods\n\nWe recruited 24 participants (12 female, mean age = 22.3 years).\nStimuli were presented using PsychoPy [@peirce2019psychopy].\n\n### Apparatus\n\nThe display was a 24-inch monitor at 60 Hz.\n\n## Results\n\nReaction times are shown in Table 1 and Figure 1.\nThat’s it. It’s just text with some punctuation conventions."
  },
  {
    "objectID": "plaintext-authoring.html#key-elements",
    "href": "plaintext-authoring.html#key-elements",
    "title": "Plaintext Authoring",
    "section": "2.2 Key Elements",
    "text": "2.2 Key Elements\n\n\n\nElement\nSyntax\nRenders as\n\n\n\n\nHeading 1\n# Title\nLarge heading\n\n\nHeading 2\n## Section\nSection heading\n\n\nHeading 3\n### Subsection\nSubsection heading\n\n\nBold\n**bold**\nbold\n\n\nItalic\n*italic*\nitalic\n\n\nInline code\n`code`\ncode\n\n\nLink\n[text](url)\nclickable link\n\n\nImage\n![caption](path.png)\nembedded image\n\n\nBlockquote\n&gt; quoted text\nindented quote\n\n\nHorizontal rule\n---\ndivider line"
  },
  {
    "objectID": "plaintext-authoring.html#extensions",
    "href": "plaintext-authoring.html#extensions",
    "title": "Plaintext Authoring",
    "section": "2.3 Extensions",
    "text": "2.3 Extensions\nPandoc (which we’ll meet next) extends basic Markdown with features essential for academic writing:\n\nCitations: [@smith2020] or @smith2020\nFootnotes: ^[This is a footnote.]\nMath: $E = mc^2$ for inline, $$\\sum_{i=1}^n x_i$$ for display\nFigure captions: ![Caption text](figure.png){width=80%}\nTables with captions\nCross-references (with filters like pandoc-crossref)\nYAML metadata blocks for title, author, date, and formatting options"
  },
  {
    "objectID": "plaintext-authoring.html#resources",
    "href": "plaintext-authoring.html#resources",
    "title": "Plaintext Authoring",
    "section": "2.4 Resources",
    "text": "2.4 Resources\n\nMarkdown basics: https://www.markdownguide.org/basic-syntax/\nPandoc’s Markdown extensions: https://pandoc.org/MANUAL.html#pandocs-markdown\nPractice: https://www.markdowntutorial.com/ (interactive, takes ~15 minutes)"
  },
  {
    "objectID": "plaintext-authoring.html#installation",
    "href": "plaintext-authoring.html#installation",
    "title": "Plaintext Authoring",
    "section": "3.1 Installation",
    "text": "3.1 Installation\n\nmacOS: brew install pandoc (via Homebrew) or download from https://pandoc.org/installing.html\nWindows (WSL/Ubuntu): sudo apt install pandoc\nOr download the latest release directly: https://github.com/jgc/pandoc/releases → actually: https://github.com/jgc/pandoc/releases — use https://pandoc.org/installing.html\n\nYou also need a LaTeX distribution for PDF output (Pandoc uses LaTeX under the hood to make PDFs):\n\nmacOS: brew install --cask mactex or the smaller brew install --cask basictex\nWSL/Ubuntu: sudo apt install texlive-full (large, ~5 GB) or sudo apt install texlive-latex-recommended texlive-fonts-recommended texlive-latex-extra (smaller)\n\nA lightweight alternative to a full LaTeX install is Tectonic, which auto-downloads only the packages it needs."
  },
  {
    "objectID": "plaintext-authoring.html#usage",
    "href": "plaintext-authoring.html#usage",
    "title": "Plaintext Authoring",
    "section": "3.2 Usage",
    "text": "3.2 Usage\n# Markdown to PDF\npandoc paper.md -o paper.pdf\n\n# Markdown to Word\npandoc paper.md -o paper.docx\n\n# Markdown to HTML\npandoc paper.md -o paper.html\n\n# Markdown to LaTeX (to inspect or edit the .tex source)\npandoc paper.md -o paper.tex\nPandoc infers the output format from the file extension."
  },
  {
    "objectID": "plaintext-authoring.html#yaml-metadata",
    "href": "plaintext-authoring.html#yaml-metadata",
    "title": "Plaintext Authoring",
    "section": "3.3 YAML Metadata",
    "text": "3.3 YAML Metadata\nAt the top of your Markdown file, you can add a YAML metadata block:\n---\ntitle: \"Reaction Time Effects of Sleep Deprivation\"\nauthor: \"Jane Smith\"\ndate: \"2025-01-15\"\nabstract: |\n  We investigated the effects of 24 hours of sleep deprivation\n  on simple and choice reaction times in 48 participants ...\nbibliography: references.bib\ncsl: apa.csl\ngeometry: margin=1in\nfontsize: 12pt\n---\nThis tells Pandoc the title, author, which bibliography file to use, which citation style (CSL) to apply, and basic page geometry — all without touching a GUI."
  },
  {
    "objectID": "plaintext-authoring.html#citations",
    "href": "plaintext-authoring.html#citations",
    "title": "Plaintext Authoring",
    "section": "3.4 Citations",
    "text": "3.4 Citations\nPandoc uses a .bib file (BibTeX format) for references. You can export .bib files from Zotero, Mendeley, Google Scholar, or write them by hand.\nA .bib entry looks like this:\n@article{smith2020sleep,\n  author  = {Smith, Jane and Doe, John},\n  title   = {Sleep deprivation and reaction time},\n  journal = {Journal of Sleep Research},\n  year    = {2020},\n  volume  = {29},\n  pages   = {e13045},\n  doi     = {10.1111/jsr.13045}\n}\nIn your Markdown, you cite it as [@smith2020sleep], and Pandoc replaces that with a formatted citation (e.g., “(Smith & Doe, 2020)”) and generates the bibliography automatically.\nTo compile with citations:\npandoc paper.md --citeproc -o paper.pdf\nThe --citeproc flag tells Pandoc to process citations. The citation style is controlled by the csl: field in your YAML header. You can download thousands of CSL styles from https://www.zotero.org/styles (APA, Vancouver, Harvard, Nature, IEEE, etc.)."
  },
  {
    "objectID": "plaintext-authoring.html#resources-1",
    "href": "plaintext-authoring.html#resources-1",
    "title": "Plaintext Authoring",
    "section": "3.5 Resources",
    "text": "3.5 Resources\n\nPandoc user’s guide: https://pandoc.org/MANUAL.html\nPandoc getting started: https://pandoc.org/getting-started.html\nCitation Style Language styles: https://www.zotero.org/styles\nBibTeX entry types: https://www.bibtex.com/e/entry-types/"
  },
  {
    "objectID": "plaintext-authoring.html#sec-exercise1",
    "href": "plaintext-authoring.html#sec-exercise1",
    "title": "Plaintext Authoring",
    "section": "3.6 Exercise 1",
    "text": "3.6 Exercise 1\n\n3.6.1 Setup\nMake sure you have Pandoc and a LaTeX distribution installed (see above).\nDownload the files/plaintext-exercises.tgz archive, move it to your Psych_9040/ directory, and then unpack it:\n$ tar -xvzf plaintext-exercises.tgz\nThis is what it looks like:\n$ tree -F plaintext-exercises\nplaintext-exercises/\n├── exercise1/\n│   ├── exercise1.md\n│   ├── refs.bib\n│   └── TASKS.md\n├── exercise2/\n│   ├── exercise2.tex\n│   ├── refs.bib\n│   ├── sample_figure.pdf\n│   ├── sample_figure.png\n│   └── TASKS.md\n├── exercise3/\n│   ├── exercise3.md\n│   ├── refs.bib\n│   ├── search_figure.pdf\n│   ├── search_figure.png\n│   └── TASKS.md\n├── README.md\n└── setup.sh\nNow run the setup.sh script which downloads a .csl file that will be used for APA-style citations:\n$ bash setup.sh\nDownloading APA 7th edition citation style...\nCopying apa.csl into exercise folders...\n  → exercise1/apa.csl\n  → exercise2/apa.csl\n  → exercise3/apa.csl\n\nDone! You're ready for the exercises.\nFor this exercise we will be using the files in the exercise1/ directory:\n\n\n3.6.2 Task\nYou are given a starter file exercise1.md (see below). Your tasks:\n\nRead the file in a text editor. Notice how readable it is even as raw text.\nCompile it to PDF: pandoc exercise1.md --citeproc -o exercise1.pdf\nCompile it to Word: pandoc exercise1.md --citeproc -o exercise1.docx\nOpen both outputs. Compare them to the source.\nAdd the following to the document:\n\nA new subsection under Methods called ### Data Analysis\nA sentence describing the analysis, citing a new reference (add it to the .bib file)\nAn inline equation: the formula for a t-statistic, t = \\frac{\\bar{X} - \\mu}{s / \\sqrt{n}}\nRecompile to PDF and verify your additions appear correctly.\n\n\nHint: writing equations in LaTeX can be strange at first, they have a syntax/grammar all their own, but once you get the hang of it, it’s actually pretty easy and very efficient. Here is some documentation: LaTeX Mathematics.\n\n\n3.6.3 Starter File\nexercise1.md\n---\ntitle: \"Effects of Caffeine on Grip Strength\"\nauthor: \"A. Student\"\ndate: \"2025-02-01\"\nbibliography: refs.bib\ncsl: apa.csl\ngeometry: margin=1in\nfontsize: 12pt\n---\n\n# Introduction\n\nCaffeine is the most widely consumed psychoactive substance in the\nworld [@fredholm1999actions]. Its effects on cognitive performance\nare well-documented, but effects on motor performance are less clear.\n\n# Methods\n\n## Participants\n\nWe recruited 30 healthy adults (15 female, mean age = 23.1 years,\nSD = 3.4) from the university community.\n\n## Procedure\n\nParticipants completed a maximal grip strength test using a hand\ndynamometer before and 45 minutes after consuming either 200 mg\ncaffeine or placebo in a double-blind design.\n\n# Results\n\nMean grip strength increased by 2.3 N (SD = 4.1) in the caffeine\ncondition and 0.4 N (SD = 3.8) in the placebo condition.\n\n# Discussion\n\nThese preliminary results suggest a small positive effect of caffeine\non grip strength, consistent with @warren2010effect.\n\n# References\n\n\n3.6.4 Starter File: refs.bib\n@article{fredholm1999actions,\n  author  = {Fredholm, Bertil B and Bättig, Karl and Holmén, Janet\n             and Nehlig, Astrid and Zvartau, Edwin E},\n  title   = {Actions of caffeine in the brain with special reference\n             to factors that contribute to its widespread use},\n  journal = {Pharmacological Reviews},\n  year    = {1999},\n  volume  = {51},\n  number  = {1},\n  pages   = {83--133}\n}\n\n@article{warren2010effect,\n  author  = {Warren, Gregory L and Park, Nicole D and Maresca,\n             Robert D and McKibans, Kimberly I and Millard-Stafford,\n             Mindy L},\n  title   = {Effect of caffeine ingestion on muscular strength and\n             endurance},\n  journal = {Medicine and Science in Sports and Exercise},\n  year    = {2010},\n  volume  = {42},\n  number  = {7},\n  pages   = {1375--1387}\n}\n\n\n3.6.5 Debrief\n\nHow does the PDF compare to the Word output?\nHow easy is it to add a citation compared to using a Word plugin?\nWhat happens when you make a typo in the citation key?"
  },
  {
    "objectID": "plaintext-authoring.html#summary-1",
    "href": "plaintext-authoring.html#summary-1",
    "title": "Plaintext Authoring",
    "section": "3.7 Summary",
    "text": "3.7 Summary\nWe learned about:\n\nWhy plaintext is a compelling choice for scientific writing\nThe basics of Markdown syntax\nHow Pandoc converts Markdown to PDF, Word, and other formats\nHow to handle citations with BibTeX and CSL\n\nIn the next section we will dive into LaTeX, which is the typesetting system that pandoc uses under the hood. We will look at how to write LaTeX directly (useful for when you need fine-grained control), how to handle equations, figures, tables, and cross-references, and how to use Overleaf for collaborative LaTeX editing. Many people write LaTeX directly, bypassing Markdown and Pandoc altogether. Historical note: I wrote my very first journal article in LaTeX back in 1994, and today it is still viewable, editable, and compiles to pdf, using free and open source tools.\nFor next time:\n\nVerify that your LaTeX distribution is working: try pdflatex --version in your terminal.\nCreate a free account on Overleaf, we will use it for an exercise.\n(Optional) Work through the interactive Markdown tutorial: https://www.markdowntutorial.com/"
  },
  {
    "objectID": "plaintext-authoring.html#whatwhy",
    "href": "plaintext-authoring.html#whatwhy",
    "title": "Plaintext Authoring",
    "section": "4.1 What/Why?",
    "text": "4.1 What/Why?\nLaTeX (pronounced “LAH-tek” or “LAY-tek”) is a typesetting system created by Leslie Lamport in 1984, built on top of Donald Knuth’s TeX (1978). Knuth, a computer scientist, created TeX because he was unhappy with the typographic quality of his textbooks. LaTeX adds a layer of convenience on top of TeX.\nWhen you compile a LaTeX document, the system makes thousands of micro-decisions about line breaks, hyphenation, spacing, and page layout. The result is typographically excellent output, the kind of polished, professional look you see in textbooks and journal articles.\nWord processors like Microsoft Word, or Google Docs, or Apple’s Pages, are written to make some typographic “decisions” but the truth is that they pale in comparison to the polished professional look you get using LaTeX.\n\n4.1.1 Pandoc & LaTeX\nWhen you ran pandoc paper.md -o paper.pdf above, Pandoc actually:\n\nConverted your Markdown to LaTeX behind the scenes\nCalled pdflatex (or xelatex) to compile the LaTeX to PDF\n\nSo you’ve already been using LaTeX. You just had Pandoc as a middleman. Learning LaTeX directly gives you much finer control when you need it. It also removes one layer of dependency/complexity from the process.\n\n\n4.1.2 Which one?\n\n\n\n\n\n\n\nScenario\nRecommendation\n\n\n\n\nShort paper, simple structure\nMarkdown + Pandoc\n\n\nBlog post, course notes, README\nMarkdown + Pandoc\n\n\nThesis or dissertation\nLaTeX directly (or Markdown for drafting, LaTeX for final)\n\n\ncomplex equations\nLaTeX directly\n\n\nprecise figure placement\nLaTeX directly\n\n\nnumbered figures, equations, tables\nLaTeX directly\n\n\nJournal provides a LaTeX template\nLaTeX directly\n\n\nNeed Word output for a journal\nMarkdown + Pandoc (convert to .docx)\n\n\nQuick first draft\nMarkdown + Pandoc\n\n\n\nMany working scientists use a hybrid: draft quickly in Markdown, then move to LaTeX when the document matures and needs fine-tuning."
  },
  {
    "objectID": "plaintext-authoring.html#document-structure",
    "href": "plaintext-authoring.html#document-structure",
    "title": "Plaintext Authoring",
    "section": "4.2 Document Structure",
    "text": "4.2 Document Structure\nA LaTeX document has two parts: the preamble (setup and configuration) and the document body (your content).\n\n4.2.1 Minimal Example\n\\documentclass[12pt]{article}\n\n% --- Preamble: load packages and configure ---\n\\usepackage[margin=1in]{geometry}\n\\usepackage{amsmath}          % better equation support\n\\usepackage{graphicx}         % for including figures\n\\usepackage{booktabs}         % for professional tables\n\\usepackage[\n  backend=biber,\n  style=apa,\n  natbib=true\n]{biblatex}                   % citation management\n\\addbibresource{refs.bib}\n\n\\title{Effects of Caffeine on Grip Strength}\n\\author{A.\\ Student}\n\\date{February 2025}\n\n% --- Document body ---\n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\nWe investigated the effect of 200\\,mg caffeine on maximal\ngrip strength in a double-blind placebo-controlled design.\n\\end{abstract}\n\n\\section{Introduction}\n\nCaffeine is the most widely consumed psychoactive substance\nin the world \\citep{fredholm1999actions}. Its effects on\ncognitive performance are well-documented, but effects on\nmotor performance are less clear.\n\n\\section{Methods}\n\n\\subsection{Participants}\n\nWe recruited 30 healthy adults (15 female, mean age = 23.1\nyears, SD = 3.4) from the university community.\n\n\\subsection{Procedure}\n\nParticipants completed a maximal grip strength test before\nand 45 minutes after consuming caffeine or placebo.\n\n\\section{Results}\n\nThe results are presented in Table~\\ref{tab:results} and\nFigure~\\ref{fig:grip}.\n\n\\section{Discussion}\n\nThese preliminary results suggest a small positive effect\nof caffeine on grip strength \\citep{warren2010effect}.\n\n\\printbibliography\n\n\\end{document}\n\n\n4.2.2 Key Concepts\nCommands start with a backslash: \\section{...}, \\textbf{...}, \\citep{...}\nEnvironments are blocks delimited by \\begin{...} and \\end{...}: \\begin{abstract}...\\end{abstract}, \\begin{figure}...\\end{figure}\nPackages extend LaTeX’s capabilities. You load them in the preamble with \\usepackage{...}. It’s a similar idea to Python libraries.\nComments start with % and are ignored by the compiler.\n\n\n4.2.3 Compiling\n# Basic compilation\npdflatex paper.tex\n\n# With bibliography (requires multiple passes)\npdflatex paper.tex\nbiber paper\npdflatex paper.tex\npdflatex paper.tex\n\n# Or use latexmk, which automates the multiple passes:\nlatexmk -pdf paper.tex\nThe multiple passes are needed because LaTeX resolves cross-references and citations iteratively. latexmk handles this automatically and is the recommended way to compile."
  },
  {
    "objectID": "plaintext-authoring.html#equations",
    "href": "plaintext-authoring.html#equations",
    "title": "Plaintext Authoring",
    "section": "4.3 Equations",
    "text": "4.3 Equations\nThis is where LaTeX truly shines. The equation syntax is the same whether you’re writing in LaTeX or in Markdown+Pandoc (because Pandoc passes math through to LaTeX).\n\n4.3.1 Inline Math\nSurround with single dollar signs:\nThe sample mean is $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$.\n\n\n4.3.2 Display Math\nUse the equation environment for numbered equations:\n\\begin{equation}\n  t = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}\n  \\label{eq:ttest}\n\\end{equation}\nYou can then refer to it with Equation~\\ref{eq:ttest}.\nFor unnumbered equations use \\[ and \\] brackets:\n\\[\n  \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\n\n\n4.3.3 Multi-Line Equations\n\\begin{align}\n  \\text{SS}_{\\text{total}} &= \\text{SS}_{\\text{between}}\n                              + \\text{SS}_{\\text{within}} \\\\\n  F &= \\frac{\\text{MS}_{\\text{between}}}\n             {\\text{MS}_{\\text{within}}}\n\\end{align}\n\n\n4.3.4 Quick Reference\n\n\n\n\n\n\n\n\nWhat you want\nLaTeX code\nRenders as\n\n\n\n\nGreek letters\n\\alpha, \\beta, \\mu, \\sigma\nα, β, μ, σ\n\n\nSubscript\nx_{i}\nxᵢ\n\n\nSuperscript\nx^{2}\nx²\n\n\nFraction\n\\frac{a}{b}\na/b (stacked)\n\n\nSquare root\n\\sqrt{n}\n√n\n\n\nSum\n\\sum_{i=1}^{n}\nΣ\n\n\nIntegral\n\\int_{0}^{\\infty}\n∫\n\n\nHat/bar\n\\hat{\\beta}, \\bar{x}\nβ̂, x̄\n\n\nApproximately\n\\approx\n≈\n\n\nLess/greater or equal\n\\leq, \\geq\n≤, ≥\n\n\nPartial derivative\n\\frac{\\partial f}{\\partial x}\n∂f/∂x\n\n\n\n\n\n4.3.5 Resources\n\nDetexify — draw a symbol and it tells you the LaTeX code: https://detexify.kirelabs.org/\nLaTeX math cheat sheet: https://katex.org/docs/supported\nThe amsmath package documentation covers everything: https://mirrors.ctan.org/macros/latex/required/amsmath/amsldoc.pdf"
  },
  {
    "objectID": "plaintext-authoring.html#figures-tables",
    "href": "plaintext-authoring.html#figures-tables",
    "title": "Plaintext Authoring",
    "section": "4.4 Figures & Tables",
    "text": "4.4 Figures & Tables\n\n4.4.1 Figures\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{figures/grip_strength.pdf}\n  \\caption{Mean grip strength (N) by condition. Error bars show\n           $\\pm 1$ SE.}\n  \\label{fig:grip}\n\\end{figure}\nKey points:\n\n[htbp] = placement preference: here, top of page, bottom, separate page. LaTeX decides the best placement. (This is a feature, not a bug; LaTeX optimizes page layout globally.)\nUse PDF or PNG for figures. PDF is preferred for vector graphics (plots), PNG for raster images (e.g. photos, microscopy).\nReference with Figure~\\ref{fig:grip} ; the number updates automatically.\nThe tilde ~ prevents a line break between “Figure” and the number.\n\n\n\n4.4.2 Tables\n\\begin{table}[htbp]\n  \\centering\n  \\caption{Mean grip strength (N) by condition and time point.}\n  \\label{tab:results}\n  \\begin{tabular}{lcc}\n    \\toprule\n    Condition & Pre & Post \\\\\n    \\midrule\n    Caffeine  & 38.2 (5.1) & 40.5 (5.3) \\\\\n    Placebo   & 37.9 (4.8) & 38.3 (5.0) \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\nKey points:\n\n{lcc} = three columns: left-aligned, center, center.\n\\toprule, \\midrule, \\bottomrule come from the booktabs package and produce clean, professional horizontal rules (no vertical lines — this is a typographic best practice).\nReference with Table~\\ref{tab:results}.\n\nPandoc Markdown tables are simpler but less flexible:\n| Condition | Pre        | Post       |\n|-----------|------------|------------|\n| Caffeine  | 38.2 (5.1) | 40.5 (5.3) |\n| Placebo   | 37.9 (4.8) | 38.3 (5.0) |\n\n: Mean grip strength (N) by condition and time point. {#tbl:results}"
  },
  {
    "objectID": "plaintext-authoring.html#exercise-2",
    "href": "plaintext-authoring.html#exercise-2",
    "title": "Plaintext Authoring",
    "section": "4.5 Exercise 2",
    "text": "4.5 Exercise 2\nIn this exercise you will practice building a LaTeX document.\nMake sure you have completed the setup described in Section 3.6.\nFor this exercise we will be using the files in the exercise2/ directory:\n\n4.5.1 Option A: Overleaf\n(Recommended for First-Timers)\n\nGo to https://www.overleaf.com/ and log in.\nCreate a new blank project.\nStart with the minimal template from Section 4.2.1 above.\nComplete these tasks:\n\nTask 1 — Equations: Add a “Data Analysis” subsection under Methods. Include the formula for Pearson’s correlation coefficient as a numbered equation:\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\nThen reference it in the text (“… as shown in Equation 1”).\nTask 2 — Table: Add a Results table with three columns (Measure, Caffeine, Placebo) and at least two rows. Use booktabs rules. Add a caption and label, and reference the table in the text.\nTask 3 — Figure: Download any plot image (or use a placeholder) and include it as a figure with a caption. Reference it in the text. (On Overleaf, use the upload button to add an image file.)\nTask 4 — Citation: Add a new entry to refs.bib and cite it somewhere in the text. Verify it appears in the bibliography.\n\n\n4.5.2 Option B: Locally\nSame tasks, but compile locally:\nlatexmk -pdf exercise2.tex\nIf you don’t have latexmk, use:\npdflatex exercise2.tex\nbiber exercise2\npdflatex exercise2.tex\npdflatex exercise2.tex\n\n\n4.5.3 Debrief\n\nHow did the equation-writing experience compare to Word’s equation editor?\nWhat happened when you changed the placement hint on your figure from [h] to [t]?\nHow did adding a citation and recompiling feel compared to managing references in a GUI?"
  },
  {
    "objectID": "plaintext-authoring.html#overleaf",
    "href": "plaintext-authoring.html#overleaf",
    "title": "Plaintext Authoring",
    "section": "4.6 Overleaf",
    "text": "4.6 Overleaf\nOverleaf deserves special mention because it addresses the most common objection to LaTeX: collaboration. Overleas is a collaborative LaTeX in the browser much like Google Docs.\n\nReal-time collaboration like Google Docs — multiple people can edit simultaneously\nNo local installation required — everything compiles in the cloud\nRich library of templates for journals, theses, CVs, posters: https://www.overleaf.com/latex/templates\nGit integration on paid plans — you can push/pull your Overleaf project to a Git repo\nTrack changes and comments — familiar to Word users\nFree tier is generous for individual use; paid plans add more collaborators\n\nMany universities provide institutional Overleaf subscriptions. Unfortunately at present Western does not.\nFor students who may not write LaTeX daily, Overleaf significantly lowers the barrier to entry. You get the typographic quality and structural rigor of LaTeX with a Google Docs-like editing experience."
  },
  {
    "objectID": "plaintext-authoring.html#a-workflow",
    "href": "plaintext-authoring.html#a-workflow",
    "title": "Plaintext Authoring",
    "section": "4.7 A Workflow",
    "text": "4.7 A Workflow\nHere are three practical workflows you might adopt:\n\n4.7.1 Workflow 1: Markdown + Pandoc (Simplest)\nBest for: drafts, short papers, course assignments, people who want minimal setup.\npaper.md + refs.bib → pandoc → paper.pdf or paper.docx\n\n\n4.7.2 Workflow 2: LaTeX on Overleaf (Collaborative)\nBest for: thesis chapters, papers with co-authors, when a journal provides a LaTeX template.\npaper.tex + refs.bib → Overleaf (cloud) → paper.pdf\n\n\n4.7.3 Workflow 3: LaTeX Locally with Git (Maximum Control)\nBest for: advanced users, large projects, integration with data analysis pipelines.\npaper.tex + refs.bib → latexmk → paper.pdf\n         ↑\n    version controlled with Git\n\n\n4.7.4 Workflow 4: Hybrid (Common in Practice)\nDraft in Markdown (fast, low friction), then convert to LaTeX when the paper matures and you need fine-tuned formatting or a journal template.\ndraft.md → pandoc → paper.tex → edit in LaTeX → paper.pdf\nAll four approaches keep your source as plaintext. All four let you use Git. All four separate content from presentation."
  },
  {
    "objectID": "plaintext-authoring.html#exercise-3",
    "href": "plaintext-authoring.html#exercise-3",
    "title": "Plaintext Authoring",
    "section": "4.8 Exercise 3",
    "text": "4.8 Exercise 3\nA Full Pipeline\n\n4.8.1 Task\nFrom a Markdown draft to polished output documents.\nYou are given a Markdown file representing a short paper. Your tasks:\n\nCompile to PDF with Pandoc. Inspect the output.\nCompile to Word with Pandoc. Open in Word. Confirm it looks reasonable.\nCompile to LaTeX with pandoc exercise3.md -o exercise3.tex. Open the .tex file and look at what Pandoc generated.\nEdit the .tex file: adjust figure placement, tweak table formatting, or add a LaTeX-specific feature like \\pagebreak.\nCompile the edited .tex to PDF: latexmk -pdf exercise3.tex\n\n\n\n4.8.2 Starter File\nexercise3.md\n---\ntitle: \"Visual Search Asymmetry in Threat Detection\"\nauthor: \"A. Student\"\ndate: \"2025-02-15\"\nabstract: |\n  We tested whether threatening stimuli (snakes) are detected\n  faster than non-threatening stimuli (flowers) in a visual\n  search task. Results supported the threat superiority effect.\nbibliography: refs3.bib\ncsl: apa.csl\ngeometry: margin=1in\nfontsize: 12pt\n---\n\n# Introduction\n\nThe threat superiority effect refers to the faster detection of\nthreatening stimuli in visual search [@ohman2001fears]. This has\nbeen demonstrated for snakes [@ohman2001fears], spiders\n[@rakison2009does], and angry faces [@fox2000facial].\n\n# Methods\n\n## Participants\n\nForty undergraduate students (28 female, mean age = 19.8,\nSD = 1.2) participated for course credit.\n\n## Stimuli and Design\n\nSearch displays contained 8 items arranged in a circle. On\ntarget-present trials, one item was the target (snake or flower)\namong 7 distractors (mushrooms). We used a 2 (target: snake,\nflower) × 2 (target presence: present, absent) design.\n\n## Data Analysis\n\nReaction times shorter than 200 ms or longer than 2000 ms were\nexcluded (1.8% of trials). We computed mean RTs for correct\nresponses and analyzed them with a repeated-measures ANOVA:\n\n$$F = \\frac{MS_{between}}{MS_{within}}$$\n\n# Results\n\nMean reaction times are shown in Table 1.\n\n| Target  | Present     | Absent      |\n|---------|-------------|-------------|\n| Snake   | 612 (89)    | 831 (102)   |\n| Flower  | 698 (94)    | 854 (110)   |\n\n: Mean reaction times in ms (SD in parentheses). {#tbl:rts}\n\nThere was a significant main effect of target type,\n$F(1, 39) = 14.2$, $p &lt; .001$, $\\eta_p^2 = .27$.\n\n# Discussion\n\nOur findings replicate the threat superiority effect\n[@ohman2001fears]. Snake targets were detected approximately\n86 ms faster than flower targets.\n\n# References\n\n\n4.8.3 Debrief\n\nYou just went from one source file to three output formats in about 60 seconds of terminal commands.\nYou then had the option to “eject” to LaTeX for fine-tuning. No information was lost.\nAsk yourself: what would it be like to do this in MS Word?"
  },
  {
    "objectID": "plaintext-authoring.html#custom-style-files",
    "href": "plaintext-authoring.html#custom-style-files",
    "title": "Plaintext Authoring",
    "section": "4.9 Custom Style Files",
    "text": "4.9 Custom Style Files\nRemember the principle from the beginning: separate content from style. LaTeX lets you take this even further with custom style files (.sty).\nAll those \\usepackage commands and configuration lines in your preamble? You can move them into a .sty file and load the whole thing with a single line. For example, suppose your preamble has:\n\\usepackage[margin=1in]{geometry}\n\\usepackage{amsmath}\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[backend=biber, style=apa, natbib=true]{biblatex}\n\\addbibresource{refs.bib}\n\\usepackage{setspace}\n\\doublespacing\n\\usepackage{lineno}\n\\linenumbers\nYou can put all of that into a file called manuscript.sty:\n% manuscript.sty — double-spaced, line-numbered, for journal submission\n\\ProvidesPackage{manuscript}\n\n\\RequirePackage[margin=1in]{geometry}\n\\RequirePackage{amsmath}\n\\RequirePackage{graphicx}\n\\RequirePackage{booktabs}\n\\RequirePackage[backend=biber, style=apa, natbib=true]{biblatex}\n\\addbibresource{refs.bib}\n\\RequirePackage{setspace}\n\\doublespacing\n\\RequirePackage{lineno}\n\\linenumbers\nThen your .tex file’s preamble shrinks to:\n\\documentclass[12pt]{article}\n\\usepackage{manuscript}\n\n\\title{Effects of Caffeine on Grip Strength}\n\\author{A.\\ Student}\n\\date{February 2025}\n\n\\begin{document}\n...\nNow make a second style file called preprint.sty that uses single spacing, no line numbers, and a different margin:\n% preprint.sty — clean single-spaced format for preprint servers\n\\ProvidesPackage{preprint}\n\n\\RequirePackage[margin=1.25in]{geometry}\n\\RequirePackage{amsmath}\n\\RequirePackage{graphicx}\n\\RequirePackage{booktabs}\n\\RequirePackage[backend=biber, style=apa, natbib=true]{biblatex}\n\\addbibresource{refs.bib}\nTo switch between submission and preprint formatting, you change one line:\n\\usepackage{manuscript}   % for journal submission\n% \\usepackage{preprint}   % for preprint/sharing\nYour content doesn’t change at all. The same .tex file produces a double-spaced, line-numbered manuscript for the journal, or a clean single-spaced preprint, depending on which .sty file you load. This is separation of content and style in practice.\nA few notes:\n\nInside .sty files, use \\RequirePackage instead of \\usepackage. They do the same thing, but \\RequirePackage is the convention for package and style files.\nThe \\ProvidesPackage{name} line should match the filename (without .sty).\nKeep the .sty files in the same directory as your .tex file (or in your local texmf tree if you want them available globally).\nJournal-provided LaTeX templates work on exactly this principle — they give you a .cls or .sty file that handles all the formatting, and you just write your content.\n\n\n4.9.1 Example: A Paper\nDownload and unpack the archive example_paper.tgz\n$ tar -xvzf example_paper.tgz\n$ tree example_paper\nexample_paper\n├── Figure1.pdf\n├── Figure2.pdf\n├── Figure3.pdf\n├── Figure4.pdf\n├── Figure5.pdf\n├── Figure6.pdf\n├── Figure7.pdf\n├── jneurophysiol.bst\n├── paper.tex\n├── plg_jnp.sty\n├── plg_min.sty\n├── plg_nice.sty\n└── refs.bib\n1 directory, 12 files\nThere is a manuscript file paper.tex, a bibtex file refs.bib, 7 Figures (Figure*.pdf), and three .sty files, which I’ve created, which styles the manuscript three different ways. The jneurophysiol.bst file is provided for us and styles the bibliography according to the J Neurophysiol style.\nOpen the paper.tex main manuscript file and you will see:\n\\documentclass{article}\n\n% MINIMAL LATEX\n\\usepackage{plg_min}\n\n% IMPROVED LATEX\n% \\usepackage{plg_nice}\n\n% MY \"J Neurophysiol classic\" PREPRINT STYLE\n% \\usepackage{plg_jnp}\n\n\n\\title{A Context-Free Model of Savings in Motor Learning}\n\n\\author[1,$\\ddagger$]{Mahdiyar Shahbazi}\n\\author[2,3]{Olivier Codol}\n\\author[4,5,$\\dagger$]{Jonathan A. Michaels}\n\\author[1,4,$\\dagger$,*]{Paul L. Gribble}\n\\affil[1]{Dept. Psychology, Western University, London, ON, Canada}\n\\affil[2]{Mila--Qu\\'ebec Artificial Intelligence Institute, Montr\\'eal, QC, Canada}\n\\affil[3]{Dept. Neuroscience, Universit\\'e de Montr\\'eal, Montr\\'eal, QC, Canada}\n\\affil[4]{Dept. Physiology \\& Pharmacology, Schulich School of Medicine \\& Dentistry, London, ON, Canada}\n\\affil[5]{School of Kinesiology and Health Science, Faculty of Health, York University, Toronto, ON, Canada}\n\\affil[$\\dagger$]{Co-senior authors}\n\\affil[$\\ddagger$]{Present address: Dept. Organismic and Evolutionary Biology, Harvard University, Boston, MA, USA}\n\\affil[*]{Corresponding author: pgribble@uwo.ca}\n\n\\date{\\normalsize\\today}\n\n\n\\begin{document}\n\n\\maketitle\n\\thispagestyle{empty}\n\n\\begin{abstract}\nLearning to adapt voluntary movements to an external perturbation, whether mechanical or visual, is \nTry compiling the document to a .pdf as it is now (with \\usepackage{plg_min} uncommented):\n$ pdflatex paper.tex\n$ bibtex paper.tex\n$ pdflatex paper.tex\n$ pdflatex paper.tex\nOpen paper.pdf and have a look.\nNow try commenting out \\usepackage{plg_min} and uncommenting \\usepackage{plg_nice}. Recompile to .pdf. See how it looks now.\nNow try commenting out \\usepackage{plg_nice} and uncommenting \\usepackage{plg_jnp}. Recompile to .pdf. See how it looks now.\nThe possibilities for styling a LaTeX document are almost endless. Anything you can think of, you can do.\n\n\n4.9.2 Example: A Letter\nAs a final example, some time ago I created a LaTeX style file that duplicates the official Western letterhead, so that I could easily write letters. As a result, I can write a letter like this:\n\\documentclass{Psychology_letter}\n\n\\begin{document}\n\n\\thedate{\\today}\n\n\\recipient{Ford Prefect\\\\\nArthur's friend\\\\\nThe Hitchhiker's Guide to the Galaxy}\n\n\\regarding{something important}\n\n\\greeting{Dear Colleague,}\n\nIsn't it great to be able to write a letter using LaTeX and \nnot have to open MS Turd, and deal with fonts, hidden menus, \nand changes in formatting, not to mention random crashes. In \naddition this plaintext file will be readable forever, using \nany text editor, on any operating system.\n\n\\closingsig{Sincerely,}\n\n\\end{document}\nA quick compile:\n$ pdflatex letter.tex\nand I get a file letter.pdf that looks like this:\n\nOnce you spend some time writing documents using LaTeX, going back and dealing with MS Turd is quite a shock to the system."
  },
  {
    "objectID": "plaintext-authoring.html#quarto-resources",
    "href": "plaintext-authoring.html#quarto-resources",
    "title": "Plaintext Authoring",
    "section": "5.1 Quarto Resources",
    "text": "5.1 Quarto Resources\n\nQuarto home: https://quarto.org/\nGetting started guide: https://quarto.org/docs/get-started/\nGallery of examples: https://quarto.org/docs/gallery/"
  },
  {
    "objectID": "plaintext-authoring.html#terminal-commands",
    "href": "plaintext-authoring.html#terminal-commands",
    "title": "Plaintext Authoring",
    "section": "6.1 Terminal Commands",
    "text": "6.1 Terminal Commands\n# Pandoc: Markdown to PDF\npandoc paper.md --citeproc -o paper.pdf\n\n# Pandoc: Markdown to Word\npandoc paper.md --citeproc -o paper.docx\n\n# Pandoc: Markdown to LaTeX\npandoc paper.md -s -o paper.tex\n\n# LaTeX: compile with automatic re-runs\nlatexmk -pdf paper.tex\n\n# LaTeX: clean up auxiliary files\nlatexmk -c"
  },
  {
    "objectID": "plaintext-authoring.html#tools",
    "href": "plaintext-authoring.html#tools",
    "title": "Plaintext Authoring",
    "section": "6.2 Tools",
    "text": "6.2 Tools\n\n\n\nTool\nWhat it does\nLink\n\n\n\n\nPandoc\nUniversal document converter\nhttps://pandoc.org/\n\n\nTeX Live\nFull LaTeX distribution (Linux/Windows)\nhttps://tug.org/texlive/\n\n\nMacTeX\nTeX Live for macOS\nhttps://tug.org/mactex/\n\n\nOverleaf\nCollaborative LaTeX editor in the browser\nhttps://www.overleaf.com/\n\n\nZotero\nReference manager (exports .bib)\nhttps://www.zotero.org/\n\n\nBetter BibTeX\nZotero plugin for clean .bib export\nhttps://retorque.re/zotero-better-bibtex/\n\n\nDetexify\nDraw a symbol → get LaTeX code\nhttps://detexify.kirelabs.org/\n\n\nTeXstudio\nLocal LaTeX editor with preview\nhttps://www.texstudio.org/\n\n\nVS Code + LaTeX Workshop\nLaTeX editing in VS Code\nhttps://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop"
  },
  {
    "objectID": "plaintext-authoring.html#further-learning",
    "href": "plaintext-authoring.html#further-learning",
    "title": "Plaintext Authoring",
    "section": "6.3 Further Learning",
    "text": "6.3 Further Learning\n\nOverleaf’s LaTeX tutorials (excellent for beginners): https://www.overleaf.com/learn\nThe Not So Short Introduction to LaTeX2e (classic free guide): search CTAN for “lshort”\nLaTeX Wikibook: https://en.wikibooks.org/wiki/LaTeX\nPandoc User’s Guide: https://pandoc.org/MANUAL.html\nLaTeX StackExchange (Q&A community): https://tex.stackexchange.com/\nBibTeX entry types and fields: https://www.bibtex.com/e/entry-types/"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to the Course",
    "section": "",
    "text": "Welcome! I’m glad you’re here.\nThis is Psychology 9040: Scientific Computing, for FW25, Jan-Apr 2026.\nBring your computer to class! We will be writing code.\nCourse website: https://gribblelab.org/9040"
  },
  {
    "objectID": "intro.html#course-goals",
    "href": "intro.html#course-goals",
    "title": "Introduction to the Course",
    "section": "Course Goals",
    "text": "Course Goals\nIn this one-semester graduate course you will learn skills in scientific computing—tools and techniques that you can use in your own research. You will learn to program using Python, which is a high-level programming language with many libraries that provide a rich ecosystem for scientific computing. If you want to use a different language in the course you are welcome to but I will focus on Python in class. Having said that, as much as I can I will teach concepts in a way that are language-agnostic.\nThe course is designed to achieve three primary goals:\n\nYou will learn to write code in a high-level language (Python)\nYou will learn to think computationally and algorithmically about data analysis\nYou will learn some common data analysis techniques, which will give you a foundation from which to learn more complex scientific computing skills to suit your own research goals"
  },
  {
    "objectID": "intro.html#topics",
    "href": "intro.html#topics",
    "title": "Introduction to the Course",
    "section": "Topics",
    "text": "Topics\nIn the first part of the course you will learn how to write code. The topics we will cover are common to any high-level language including Python, MATLAB, R, Javascript, C, Julia, Go, Swift, etc. All high level languages have things like numbers, strings, arrays, loops, if-statements. Languages differ in their syntax, in the names of common functions, and sometimes in some other subtle ways (e.g. passing by reference vs passing by value) but otherwise, all high-level languages basically work in the same way. Learning these fundamental concepts of high-level programming languages using one language will enable you to learn other languages as well in the future.\nThe other aspect of programming that you will learn is how to think algorithmically about solving problems and completing tasks with computers. In general there are two reasons to use a computer to perform a given task, (1) because the task would take too long by hand (e.g. churning through processing a huge amount of data), and (2) because the computer can do something clever that you cannot do yourself (e.g. applying complex algorithms to a dataset).\nIn learning the fundamentals of high-level programming languages (data types, loops, conditionals, etc) we will sometimes practice writing code to solve little toy problems. This will start to teach you how to think algorithmically. Think about it like learning to play a musical instrument. You don’t start by learining to play Beethoven. You start learning basic skills by playing scales, arpeggios, different key signatures, etc. Building up basic skills using toy problems is a convenient path towards using coding skills to solve real-world programming problems in the context of your own thesis research.\nWe will also practice and learn by using code to answer questions about data. Some of the data will be from real experiments and some will be fictitious. The goal will be to answer a series of questions by writing code that performs operations on the data (reorganization, summarising, counting, calculating, processing, plotting, etc) and writing a short summary of your findings. This mimics how you will be using your coding skills in your own research going forward.\n\nFundamentals of Coding\n\ndigital representation of data\nbasic data types, operators, & expressions\ncontrol flow & conditionals\nfunctions\ncomplex data types\nfile input & output\ngraphical displays of data\nobject oriented programming (OOP)\n\n\n\nReproducibility & Replicability\n\nPython venvs (virtual environments)\nCode versioning using Git & GitHub\norganization of code and data\ndata analysis workflow\n\n\n\nTopics in Data Analysis\n\nsampling, signal processing, & filtering data\nstatistical tests (parametric vs resampling/boostrapping/randomization)\nfitting models to data\nsimulating dynamical systems\n\nBy building upon the foundational knowledge and skills you have acquired in this course, you will be able to learn other techniques in programming and in data analysis as your scientific research progresses."
  },
  {
    "objectID": "hw/hw07.html",
    "href": "hw/hw07.html",
    "title": "Homework 7",
    "section": "",
    "text": "Due: Mar 8 by 11:59 pm eastern standard time\nSubmit your source file (paper.md or paper.tex), your refs.bib file, and the compiled PDF to Brightspace/OWL."
  },
  {
    "objectID": "hw/hw07.html#background",
    "href": "hw/hw07.html#background",
    "title": "Homework 7",
    "section": "Background",
    "text": "Background\nYou have been given a Word document (convert_this.docx) containing a short scientific paper about sleep restriction and postural sway. It’s the kind of document a student might produce in Word: functional, but fragile.\nOpen it in Word, LibreOffice, or Google Docs and take a look. Then examine it critically. You’ll notice several problems that are typical of Word-based scientific writing:\n\nHeadings are faked with bold/large text rather than structural heading styles\nCitations are manually typed strings, not linked to a reference manager\nThe equation is typed as plain monospaced text, not a real equation\nThe reference list is typed by hand with inconsistent formatting across entries (some have DOIs, some don’t; one uses Vancouver style instead of APA; one is missing the hanging indent; one is missing a period)\nThe table uses heavy gridlines rather than clean formatting\n\nNone of these would survive a format change, a journal resubmission, or the passage of time particularly well."
  },
  {
    "objectID": "hw/hw07.html#your-task",
    "href": "hw/hw07.html#your-task",
    "title": "Homework 7",
    "section": "Your Task",
    "text": "Your Task\nRecreate this document, either in Markdown+Pandoc, or in LaTeX (your choice), fixing all of the problems above along the way.\nSpecifically, your converted document must include:\n\n1. Proper structure\n\nUse real headings (# / ## in Markdown, or \\section{} / \\subsection{} in LaTeX)\nInclude YAML metadata (Markdown) or a proper preamble (LaTeX) with title, author, and date\n\n\n\n2. Real citations and bibliography\n\nCreate a refs.bib file containing all five references from the paper\nReplace every manually typed citation with a proper citation key (e.g., [@cappuccio2010sleep] in Markdown or \\citep{cappuccio2010sleep} in LaTeX)\nThe bibliography should be generated automatically in APA format\n\n\nTip: You can find BibTeX entries on Google Scholar → click Cite → click BibTeX. Or search for the paper on https://scholar.google.com and export the entry.\n\n\n\n3. Proper equations\nReplace the plain-text equation with real LaTeX math. The document contains:\n\nCohen’s d (display equation)\nThe pooled standard deviation formula (inline)\n\nThese should become something like:\n$$d = \\frac{M_1 - M_2}{SD_{\\text{pooled}}}$$\nand\n$SD_{\\text{pooled}} = \\sqrt{\\frac{SD_1^2 + SD_2^2}{2}}$\n\n\n4. A clean table\nRecreate the results table using proper markup:\n\nIn Markdown: a pipe table with a caption\nIn LaTeX: a tabular inside a table environment with booktabs rules (\\toprule, \\midrule, \\bottomrule — no vertical lines)\n\n\n\n5. Compile to PDF\n\nMarkdown: pandoc paper.md --citeproc -o paper.pdf\nLaTeX: latexmk -pdf paper.tex"
  },
  {
    "objectID": "hw/hw07.html#what-to-submit",
    "href": "hw/hw07.html#what-to-submit",
    "title": "Homework 7",
    "section": "What to Submit",
    "text": "What to Submit\n\nYour source file (paper.md or paper.tex)\nYour bibliography file (refs.bib)\nYour compiled output (paper.pdf)"
  },
  {
    "objectID": "hw/hw07.html#grading",
    "href": "hw/hw07.html#grading",
    "title": "Homework 7",
    "section": "Grading",
    "text": "Grading\nThis is a completion-based assignment. Full marks if:\n\nThe document compiles to PDF without errors\nHeadings are structurally correct (not faked with bold text)\nAll five citations use BibTeX keys and the bibliography is auto-generated\nThe equations are rendered in LaTeX math\nThe table is well formatted"
  },
  {
    "objectID": "hw/hw07.html#hints",
    "href": "hw/hw07.html#hints",
    "title": "Homework 7",
    "section": "Hints",
    "text": "Hints\n\nYou do not need to match the Word document’s appearance exactly. The point is to produce a better version using plaintext tools.\nIf using Markdown+Pandoc, remember to include --citeproc and to specify bibliography: refs.bib and csl: apa.csl in your YAML header.\nIf using LaTeX, biblatex with biber and style=apa will handle APA formatting.\nThe APA CSL file can be downloaded from https://www.zotero.org/styles/apa.\nIf you are skilled with these kinds of tools this assignment should take you about 15 minutes. If you’re spending much longer, ask for help. If you haven’t read through the course notes on (plaintext authoring) and completed the exercises, do that first."
  },
  {
    "objectID": "hw/hw05.html",
    "href": "hw/hw05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Due: Feb 15 by 11:59 pm eastern standard time\nSubmit a single file called name_05.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_05.py\n\nWrite a Python program to complete the XOR Decryption exercise.\nHint: the plaintext message contains the words the, and, and that"
  },
  {
    "objectID": "hw/hw03.html",
    "href": "hw/hw03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Due: Feb 1 by 11:59 pm eastern standard time\nSubmit a single file called name_03.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_03.py\n\nWrite a Python program to complete the Square Digit Chains exercise.\nHint: the answer is a 7 digit number starting with 8 and ending in 6 … and if you add all the digits up you get 33."
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Due: Jan 18 by 11:59 pm eastern standard time\nSubmit a single file called name_01.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_01.py\n\nWrite some code to complete the Parabolic Flight coding exercise. Make sure it produces the correct output given the example inputs, including the same number of decimal places."
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "Graphics & Figures",
    "section": "",
    "text": "Read Chapter 9, Plotting and Visualization of Python for Data Analysis by Wes McKinney, for a great introduction to the kinds of plots you can make, and a basic subset of plotting commands. It’s a great place to start.\nPython has several libraries that can be used for generating graphics. The most popular ones are matplotlib and seaborn.\n\nMatplotlib: tutorials\nThe Python Graph Gallery\nSeaborn: statistical data visualization\n\nMatplotlib is more or less a copy of MATLAB’s plotting capabilities, so if you are familiar with MATLAB, you will feel at home with matplotlib.\nSeaborn is a high-level interface for drawing attractive and informative statistical graphics. It’s a little bit like ggplot2 in R. It’s great for plotting data that’s in a tabular format."
  },
  {
    "objectID": "fundamentals.html",
    "href": "fundamentals.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Digital representation of data\nControl flow & Complex data types\nFunctions, File input & output"
  },
  {
    "objectID": "fundamentals.html#pauls-notes",
    "href": "fundamentals.html#pauls-notes",
    "title": "Fundamentals",
    "section": "",
    "text": "Digital representation of data\nControl flow & Complex data types\nFunctions, File input & output"
  },
  {
    "objectID": "fundamentals.html#python-fundamentals",
    "href": "fundamentals.html#python-fundamentals",
    "title": "Fundamentals",
    "section": "Python Fundamentals",
    "text": "Python Fundamentals\n\nLearning with Python 3 chapter 1: The way of the program\nLearning with Python 3 chapter 2: Variables, expressions and statements\nPython for Data Analysis chapter 1: Preliminaries\nPython for Data Analysis chapter 2: Python Language Basics, iPython, and Jupyter Notebooks\nNumPy: the absolute basics for beginners\nPython for Data Analysis chapter 4: NumPy Basics: Arrays and Vectorized Computation\na fun coding challenge: Advent of Code 2015, Day 6 (hint: use a 1000x1000 2D array)\n\n\nother useful readings\n\nThe way of the program\nVariables, expressions, and statements\nPython Language Basics\nFiles & the File System by Kieran Healy\nHow to Name Files video by Jenny Bryan\n\n\n\nConditionals, Iteration, & Control Flow\n\nLearning with Python 3 chapter 5: Conditionals\nLearning with Python 3 chapter 7: Iteration\nPython for Data Analysis chapter 3: Built-in Data Structures, Functions, and Files\nPython for Data Analysis chapter 4: NumPy Basics: Arrays and Vectorized Computation\nPython for Data Analysis chapter 5: Getting Started with pandas\nConditionals and recursion (you can ignore for now the section on recursion)\nControl Flow\n\n\n\nFile i/o\n\nNumPy reading & writing files\nNumPy data types\npandas IO tools\nPython reading and writing files\nPython for Data Analysis chapter 6: Data Loading, Storage, and File Formats\nProject Structure, file naming, folders, etc… by Danielle Navarro"
  },
  {
    "objectID": "digital_representation_of_data.html",
    "href": "digital_representation_of_data.html",
    "title": "Digital Representation of Data",
    "section": "",
    "text": "Learning with Python 3 chapter 1: The way of the program\nLearning with Python 3 chapter 2: Variables, expressions and statements\nPython for Data Analysis chapter 1: Preliminaries\nPython for Data Analysis chapter 2: Python Language Basics, iPython, and Jupyter Notebooks\n\n\n\n\nThe way of the program\nVariables, expressions, and statements\nPython Language Basics\nHolding a Program in One’s Head"
  },
  {
    "objectID": "digital_representation_of_data.html#high-level-vs-low-level-languages",
    "href": "digital_representation_of_data.html#high-level-vs-low-level-languages",
    "title": "Digital Representation of Data",
    "section": "High-level vs low-level languages",
    "text": "High-level vs low-level languages\nThe CPU (central processing unit) chip(s) that sit on the motherboard of your computer is the piece of hardware that actually executes instructions. A CPU only understands a relatively low-level language called machine code. Often machine code is generated automatically by translating code written in assembly language, which is a low-level programming language that has a relatively direcy relationship to machine code (but is more readable by a human). A utility program called an assembler is what translates assembly language code into machine code.\nIn this course we will be learning how to program in Python, which is a high-level programming language. The “high-level” refers to the fact that the language has a strong abstraction from the details of the computer (the details of the machine code). A “strong abstraction” means that one can operate using high-level instructions without having to worry about the low-level details of carrying out those instructions.\nAn analogy is motor skill learning. A high-level language for human action might be drive your car to the grocery store and buy apples. A low-level version of this might be something like: (1) walk to your car; (2) open the door; (3) start the ignition; (4) put the transmission into Drive; (5) step on the gas pedal, and so on. An even lower-level description might involve instructions like: (1) activate your gastrocnemius muscle until you feel 2 kg of pressure on the underside of your right foot, maintain this pressure for 2.7 seconds, then release (stepping on the gas pedal); (2) move your left and right eyeballs 27 degrees to the left (check for oncoming cars); (3) activate your pectoralis muscle on the right side of your chest and simultaneously squeeze the steering wheel with the fingers on your right hand (steer the car to the left); and so on.\nFor scientific programming, we would like to deal at the highest level we can, so that we can avoid worrying about the low-level details. We might for example want to plot a line in a Figure and colour it blue. We don’t want to have to program the low-level details of how each pixel on the screen is set, and how to generate each letter of the font that is used to specify the x-axis label.\nAs an example, here is a hello, world program written in a variety of languages, just to give you a sense of things. You can see the high-level languages like MATLAB, Python and R are extremely readable and understandable, even though you may not know anything about these languages (yet). The C code is less readable, there are lots of details one may not know about... and the assembly language example is a bit of a nightmare, obviously too low-level for our needs here.\nPython\nprint(\"hello, world\")\nMATLAB\ndisp('hello, world')\nR\ncat(\"hello, world\\n\")\nJavascript\ndocument.write(\"hello, world\");\nFortran\nprint *,\"hello, world\"\nC\n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) {\n  printf(\"hello, world\\n\");\n  return 0;\n}\n8086 Assembly language\n; this example prints out  \"hello world!\" by writing directly to video memory.\n; first byte is ascii character, next is character attribute (8 bit value)\n; high 4 bits set background color and low 4 bits set foreground color.\n \norg 100h\n\n; set video mode    \nmov ax, 3     ; text mode 80x25, 16 colors, 8 pages (ah=0, al=3)\nint 10h       ; do it!\n\n; cancel blinking and enable all 16 colors:\nmov ax, 1003h\nmov bx, 0\nint 10h\n\n; set segment register:\nmov     ax, 0b800h\nmov     ds, ax\n\n; print \"hello world\"\n\nmov [02h], 'H'\nmov [04h], 'e'\nmov [06h], 'l'\nmov [08h], 'l'\nmov [0ah], 'o'\nmov [0ch], ','\nmov [0eh], 'W'\nmov [10h], 'o'\nmov [12h], 'r'\nmov [14h], 'l'\nmov [16h], 'd'\nmov [18h], '!'\n\n; color all characters:\nmov cx, 12  ; number of characters.\nmov di, 03h ; start from byte after 'h'\n\nc:  mov [di], 11101100b   ; light red(1100) on yellow(1110)\n    add di, 2 ; skip over next ascii code in vga memory.\n    loop c\n\n; wait for any key press:\nmov ah, 0\nint 16h\n\nret"
  },
  {
    "objectID": "digital_representation_of_data.html#interpreted-vs-compiled-languages",
    "href": "digital_representation_of_data.html#interpreted-vs-compiled-languages",
    "title": "Digital Representation of Data",
    "section": "Interpreted vs compiled languages",
    "text": "Interpreted vs compiled languages\nSome languages like C and Fortran are compiled languages, meaning that we write code in C or Fortran, and then to run the code (to have the computer execute those instructions) we first have to translate the code into machine code, and then run the machine code. The utility function that performs this translation (compilation) is called a compiler. In addition to simply translating a high-level language into machine code, modern compilers will also perform a number of optimizations to ensure that the resulting machine code runs fast, and uses little memory. Typically we write a program in C, then compile it, and if there are no errors, we then run it. We deal with the entire program as a whole. Compiled program tend to be fast since the entire program is compiled and optimized as a whole, into machine code, and then run on the CPU as a whole.\nOther languages, like MATLAB, Python and R, are interpreted languages, meaning that we write code which is then translated, command by command, into machine language instructions which are run one after another. This is done using a utility called an interpreter. We don’t have to compile the whole program all together in order to run it. Instead we can run it one instruction at a time. Typically we do this in an interactive programming environment where we can type in a command, and observe the result, and then type a next command, etc. This is known as the read-eval-print (REPL) loop. This is advantageous for scientific programming, where we typically spend a lot of time exploring our data in an interactive way. One can of course run a program such as this in a batch mode, all at once, without the interactive REPL environment... but this doesn’t change the fact that the translation to machine code still happens one line at a time, each in isolation. Interpreted languages tend to be slow, because every single command is taken in isolation, one after the other, and in real time translated into machine code which is then executed in a piecemeal fashion.\nFor interactive programming, when we are exploring our data, interpreted languages like MATLAB, Python and R shine. They may be slow but it (typically) doesn’t matter, because what’s many orders of magnitude slower, is the firing of the neurons in our brain as we consider the output of each command and decide what to do next, how to analyse our data differently, what to plot next, etc. For batch programming (for example fMRI processing pipelines, or electrophysiological recording signal processing, or numerical optimizations, or statistical bootstrapping operations), where we want to run a large set of instructions all at once, without looking at the result of each step along the way, compiled languages really shine. They are much faster than interpreted languages, often several orders of magnitude faster. It’s not unusual for even a simple program written in C to run 100x or even 1000x faster than the same program written in MATLAB, Python or R.\nA 1000x speedup may not be very important when the program runs in 5 seconds (versus 5 milliseconds) but when a program takes 60 seconds to run in MATLAB, Python, or R, for example, things can start to get problematic.\nImagine you write some code to read in data from one subject, process that data, and write the result to a file, and that operation takes 60 seconds. Is that so bad? Not if you only have to run it once. Now let’s imagine you have 15 subjects in your group. Now 60 seconds is 15 minutes. Now let’s say you have 4 groups. Now 15 minutes is one hour. You run your program, go have lunch, and come back an hour later and you find there was an error. You fix the error and re-run. Another hour. Even if you get it right, now imagine your supervisor asks you to re-run the analysis 5 different ways, varying some parameter of the analysis (maybe filtering the data at a different frequency, for example). Now you need 5 hours to see the result. It doesn’t take a huge amount of data to run into this sort of situation.\nNow imagine if you could program this data processing pipeline in C instead, and you could achieve a 500x speedup (not unusual), now those 5 hours turn into 36 seconds (you could run your analysis twice and it would still take less time than listening to Stairway to Heaven a dozen times). All of a sudden it’s the difference between an overnight operation and a 30 second operation. That makes a big difference to the kind of work you can do, and the kinds of questions you can pursue.\nPython (when using NumPy) and MATLAB is pretty good about using optimized, compiled subroutines for certain operations (e.g. matrix algebra), so in many cases the difference between Python/MATLAB and C performance isn’t as great as it is for others. Python has add-ons, for example Numba, that with some work, enables one to essentially compile parts of Python code. In practice this can be tricky though. MATLAB also has a toolbox (called the MATLAB Coder) that will allow you to generate C code from your MATLAB code, so in principle you can take slow MATLAB code and generate faster, compiled C code."
  },
  {
    "objectID": "digital_representation_of_data.html#readings-1",
    "href": "digital_representation_of_data.html#readings-1",
    "title": "Digital Representation of Data",
    "section": "Readings",
    "text": "Readings\n\nplay with the IEEE-754 Floating Point Converter (e.g. lookup the representation of 0.1, 0.2, and 0.3)"
  },
  {
    "objectID": "digital_representation_of_data.html#binary",
    "href": "digital_representation_of_data.html#binary",
    "title": "Digital Representation of Data",
    "section": "Binary",
    "text": "Binary\nInformation on a digital computer is stored in a binary format. Binary format represents information using a series of 0s and 1s. If there are n digits of a binary code, one can represent 2^{n} bits of information.\nSo for example the binary number denoted by:\n0001\nrepresents the number 1.\n0010\nThis is a 4-bit code since there are 4 binary digits. The full list of all values that can be represented using a 4-bit code are shown in the Table below:\n\n\nCode\nprint(\"Decimal Binary\")\nprint(\"------- -------\")\nfor n in range(16):\n    print(f\"{n:7d} {n:04b}\")\n\n\nDecimal Binary\n------- -------\n      0 0000\n      1 0001\n      2 0010\n      3 0011\n      4 0100\n      5 0101\n      6 0110\n      7 0111\n      8 1000\n      9 1001\n     10 1010\n     11 1011\n     12 1100\n     13 1101\n     14 1110\n     15 1111\n\n\nSo with a 4-bit binary code one can represent 2^{4} = 16 different values (0-15). Each additional bit doubles the number of values one can represent. So a 5-bit code enables us to represent 32 distinct values, a 6-bit code 64, a 7-bit code 128 and an 8-bit code 256 values (0-255).\nAnother piece of terminology: a given sequence of binary digits that forms the natural unit of data for a given processor (CPU) is called a word.\nHave a look at the ASCII table. The standard ASCII table represents 128 different characters and the extended ASCII codes enable another 128 for a total of 256 characters. How many binary bits are used for each?"
  },
  {
    "objectID": "digital_representation_of_data.html#hexadecimal",
    "href": "digital_representation_of_data.html#hexadecimal",
    "title": "Digital Representation of Data",
    "section": "Hexadecimal",
    "text": "Hexadecimal\nYou will also see in the ASCII table that it gives the decimal representation of each character but also the Hexadecimal and Octal representations. The hexadecimal system is a base-16 code and the octal system is a base-8 code. Hex values for a single hexadecimal digit can range over:\n0 1 2 3 4 5 6 7 8 9 a b c d e f\nIf we use a 2-digit hex code we can represent 16^{2} = 256 distinct values. In computer science, engineering and programming, a common practice is to represent successive 4-bit binary sequences using single-digit hex codes:\n\n\nCode\nprint(\"Dec Bin  Hex\")\nprint(\"--- ---- ---\")\nfor n in range(16):\n    print(f\"{n:3d} {n:04b} {n:x}\")\n\n\nDec Bin  Hex\n--- ---- ---\n  0 0000 0\n  1 0001 1\n  2 0010 2\n  3 0011 3\n  4 0100 4\n  5 0101 5\n  6 0110 6\n  7 0111 7\n  8 1000 8\n  9 1001 9\n 10 1010 a\n 11 1011 b\n 12 1100 c\n 13 1101 d\n 14 1110 e\n 15 1111 f\n\n\nIf we have 8-bit binary codes we would use successive hex digits to represent each 4-bit word of the 8-bit byte (another piece of lingo):\n\n\nCode\nprint(\"Dec Bin      Hex\")\nprint(\"--- -------- ---\")\nfor n in range(3):\n    print(f\"{n:3d} {n:08b} {n:x}\")\nprint(\"    ...     \")\nfor n in range(253,256,1):\n    print(f\"{n:3d} {n:08b} {n:x}\")\n\n\nDec Bin      Hex\n--- -------- ---\n  0 00000000 0\n  1 00000001 1\n  2 00000010 2\n    ...     \n253 11111101 fd\n254 11111110 fe\n255 11111111 ff\n\n\nThe left chunk of 4-bit binary digits (the left word) is represented in hex as a single hex digit (0-f) and the next chunk of 4-bit binary digits (the right word) is represented as another single hex digit (0-f).\nHex is typically used to represent bytes (8-bits long) because it is a more compact notation than using 8 binary digits (hex uses just 2 hex digits)."
  },
  {
    "objectID": "digital_representation_of_data.html#floating-point-values",
    "href": "digital_representation_of_data.html#floating-point-values",
    "title": "Digital Representation of Data",
    "section": "Floating point values",
    "text": "Floating point values\nThe material above talks about the decimal representation of bytes in terms of integer values (e.g. 0-255). Frequently however in science we want the ability to represent real numbers on a continuous scale, for example 3.14159, or 5.5, or 0.123, etc. For this, the convention is to use floating point representations of numbers.\nThe idea behind the floating point representation is that it allows us to represent an approximation of a real number in a way that allows for a large number of possible values. Floating point numbers are represented to a fixed number of significant digits (called a significand) and then this is scaled using a base raised to an exponent:\ns~\\mathrm{x}~b^{e}\nThis is related to something you may have come across in high-school science, namely scientific notation. In scientific notation, the base is 10 and so a real number like 123.4 is represented as 1.234~\\mathrm{x}~10^{2}.\nIn computers there are different conventions for different CPUs but there are standards, like the IEEE 754 floating-point standard. As an example, a so-called single-precision floating point format is represented in binary (using a base of 2) using 32 bits (4 bytes) and a /double precision/ floating point number is represented using 64 bits (8 bytes). In C you can find out how many bytes are used for various types using the sizeof() function:\n#include &lt;stdio.h&gt;\nint main(int argc, char *argv[]) {\n  printf(\"a single precision float uses %ld bytes\\n\", sizeof(float));\n  printf(\"a double precision float uses %ld bytes\\n\", sizeof(double));\n  return 0;\n}\nOn my macbook pro laptop this results in this output:\na single precision float uses 4 bytes\na double precision float uses 8 bytes\nAccording to the IEEE 754 standard, a single precision 32-bit binary floating point representation is composed of a 1-bit sign bit (signifying whether the number is positive or negative), an 8-bit exponent and a 23-bit significand. See the various wikipedia pages for full details.\n\nFloating point error\nThere is a key phrase in the description of floating point values above, which is that floating point representation allows us to store an approximation of a real number. If we attempt to represent a number that has more significant digits than can be store in a 32-bit floating point value, then we have to approximate that real number, typically by rounding off the digits that cannot fit in the 32 bits. This introduces rounding error.\nNow with 32 bits, or even 64-bits in the case of double precision floating point values, rounding error is likely to be relatively small. However it’s not zero, and depending on what your program is doing with these values, the rounding errors can accumulate (for example if you’re simulating a dynamical system over thousands of time steps, and at each time step there is a small rounding error).\nWe don’t need a fancy simulation however to see the results of floating point rounding error. Open up your favourite programming language (MATLAB, Python, R, C, etc) and type the following (adjust the syntax as needed for your language of choice):\n\n(0.1 + 0.2) == 0.3\n\nFalse\n\n\nHow could this return False when it ought to be true?\nWhat’s going on here? What’s happening is that these decimal numbers, 0.1, 0.2 and 0.3 are being represented by the computer in a binary floating-point format, that is, using a base 2 representation. The issue is that in base 2, the decimal number 0.1 cannot be represented precisely, no matter how many bits you use. Plug in the decimal number 0.1 into an online binary/decimal/hexadecimal converter (such as here) and you will see that the binary representation of 0.1 is an infinitely repeating sequence:\n0.000110011001100110011001100... (base 2)\nThis shouldn’t be an unfamiliar situation, if we remember that there are also real numbers that cannot be represented precisely in decimal format, either, because they involve an infintely repeating sequence. For example the real number \\frac{1}{3} when represented in decimal is:\n0.3333333333... (base 10)\nIf we try to represent \\frac{1}{3} using n decimal digits then we have to chop off the digits to the right that we cannot include, thereby rounding the number. We lose some amount of precision that depends on how many significant digits we retain in our representation.\nSo the same is true in binary. There are some real numbers that cannot be represented precisely in binary floating-point format.\nSee here for some examples of significant adverse events (i.e. disasters) cause by numerical errors.\nRounding can be used to your advantage, if you’re in the business of stealing from people (see salami slicing). In the 1980s movie Superman III, Richard Pryor’s character plays a “bumbling computer genius” who embezzles a ton of money by stealing a large number of fractions of cents (which in the movie are said to be lost anyway due to rounding) from his company’s payroll (YouTube clip here).\nThere is a comprehensive theoretical summary of these issues here: What Every Computer Scientist Should Know About Floating-Point Arithmetic.\nHere is a fantastic blog post that takes you through how floating-point numbers are represented:\nExposing Floating Point\nFinally here is a recent post by Julia Evans in which she discusses different Examples of floating point problems\nand another post by Julia Evans in which she goes through the actual floating-point arithmetic that underlies the 0.1 + 0.2 == 0.3 problem: Why does 0.1 + 0.2 = 0.30000000000000004?"
  },
  {
    "objectID": "digital_representation_of_data.html#integer-overflow",
    "href": "digital_representation_of_data.html#integer-overflow",
    "title": "Digital Representation of Data",
    "section": "Integer Overflow",
    "text": "Integer Overflow\nJust in case you thought that floating point values are the only source of problems, representing integer values also comes with the problem of integer overflow. This is when one attempts to represent an integer that is larger than possible given the number of bits available.\nSo for example if we were representing positive integers using only 16 bits, we would only be able to store 2^{16}=65536 distinct values. So if the first value is 0 then we are able to store positive integers up to 65535. If we attempt to add the value 1 to a variable that uses 16 bits and is currently storing the value 65535, the variable will “overflow”, probably back to zero, in this case.\nHere is a not-well-enough-known recent case of integer overflow error affecting Boeing’s new 787 “Dreamliner” aircraft:\nReboot Your Dreamliner Every 248 Days To Avoid Integer Overflow"
  },
  {
    "objectID": "digital_representation_of_data.html#floating-point-precision",
    "href": "digital_representation_of_data.html#floating-point-precision",
    "title": "Digital Representation of Data",
    "section": "Floating point precision",
    "text": "Floating point precision\nOne non-intuitive feature of floating point representations is that the precision varies with the magnitude of the number being represented. That is, the “next possible representable number” is a very small step away from the current number, when the number is relatively small… but it becomes very large indeed when the numbers are large, sitting far along the number line.\nIn Python the numpy package has a function called nextafter() that will report the next representable value from a given value towards a second value:\n\nimport numpy as np\nx = 1.234\nx2 = np.nextafter(x, +np.inf)\nprint(f\"smallest possible increment after {x} is\\n {x2-x:0.20f}\")\n\nsmallest possible increment after 1.234 is\n 0.00000000000000022204\n\n\nNow let’s try this with a larger number:\n\nimport numpy as np\nx = 1234567890.123\nx2 = np.nextafter(x, +np.inf)\nprint(f\"smallest possible increment after {x} is\\n {x2-x:0.20f}\")\n\nsmallest possible increment after 1234567890.123 is\n 0.00000023841857910156\n\n\nNow let’s try a much larger number:\n\nimport numpy as np\nx = 1.234 * 10**25\nx2 = np.nextafter(x, +np.inf)\nprint(f\"smallest possible increment after {x} is\\n {x2-x:0.1f}\")\n\nsmallest possible increment after 1.2340000000000001e+25 is\n 2147483648.0\n\n\nSo if x is 1.234 * 10**25 (admittedly a large number) then the next number that is possible to represent with floating point arithmetic is more than two billion! That’s a big “step” along the number line.\nThis is a consequence of the floating-point representation of numbers. If you are regularly dealing with very large numbers then you should be aware of this."
  },
  {
    "objectID": "digital_representation_of_data.html#size-of-python-built-in-types",
    "href": "digital_representation_of_data.html#size-of-python-built-in-types",
    "title": "Digital Representation of Data",
    "section": "Size of Python built-in types",
    "text": "Size of Python built-in types\nIn Python we can query the size (in bytes) of a given variable using the function getsizeof() which is part of the sys module:\n\nimport sys\n\na = int(12)\nprint(f\"the {type(a)} {a} uses {sys.getsizeof(a)} bytes\")\n\nb = 123.456\nprint(f\"the {type(b)} {b} uses {sys.getsizeof(b)} bytes\")\n\nthe &lt;class 'int'&gt; 12 uses 28 bytes\nthe &lt;class 'float'&gt; 123.456 uses 24 bytes\n\n\nIn Python (version 3 and above) integer variables start off using a certain number of bytes but if necessary they will expand.\n\na = 1234567890987654321234567891\nprint(f\"the {type(a)} {a} uses {sys.getsizeof(a)} bytes\")\n\nb = a * 10\nprint(f\"the {type(b)} {b} uses {sys.getsizeof(b)} bytes\")\n\nthe &lt;class 'int'&gt; 1234567890987654321234567891 uses 36 bytes\nthe &lt;class 'int'&gt; 12345678909876543212345678910 uses 40 bytes\n\n\nOf course there are limits governed for integers by the size of your system’s memory.\nIn the case of floating-point values, the limit of 64 bits for the IEEE double-precision floating point format:\n\nprint(sys.float_info.max)\n\n1.7976931348623157e+308"
  },
  {
    "objectID": "digital_representation_of_data.html#ascii",
    "href": "digital_representation_of_data.html#ascii",
    "title": "Digital Representation of Data",
    "section": "ASCII",
    "text": "ASCII\nASCII stands for American Standard Code for Information Interchange. ASCII codes delineate how text is represented in digital format for computers (as well as other communications equipment).\nASCII uses a 7-bit binary code to represent 128 specific characters of text. The first 32 codes (decimal 0 through 31) are non-printable codes like TAB, BEL (play a bell sound), CR (carriage return), etc. Decimal codes 32 through 47 are more typical text symbols like # and &. Decimal codes 48 through 57 are the numbers 0 through 9:\n\n\nCode\nprint(\"Dec Hex Oct Chr\")\nprint(\"--- --- --- ---\")\nfor n in range(48,58,1):\n    print(f\"{n:3d}  {n:x}  {n:o}   {chr(n)}\")\n\n\nDec Hex Oct Chr\n--- --- --- ---\n 48  30  60   0\n 49  31  61   1\n 50  32  62   2\n 51  33  63   3\n 52  34  64   4\n 53  35  65   5\n 54  36  66   6\n 55  37  67   7\n 56  38  70   8\n 57  39  71   9\n\n\nDecimal codes 65 through 90 are capital letters A through Z, and codes 97 through 122 are lowercase letters a through z:\n\n\nCode\nprint(\"Dec Hex Oct Chr      Dec Hex Oct Chr\")\nprint(\"--- --- --- ---      --- --- --- ---\")\nfor n in range(65,91,1):\n    print(f\"{n:3d}  {n:x} {n:o}   {chr(n)}      {n+32:3d}  {n+32:x} {n+32:o}   {chr(n+32)}\")\n\n\nDec Hex Oct Chr      Dec Hex Oct Chr\n--- --- --- ---      --- --- --- ---\n 65  41 101   A       97  61 141   a\n 66  42 102   B       98  62 142   b\n 67  43 103   C       99  63 143   c\n 68  44 104   D      100  64 144   d\n 69  45 105   E      101  65 145   e\n 70  46 106   F      102  66 146   f\n 71  47 107   G      103  67 147   g\n 72  48 110   H      104  68 150   h\n 73  49 111   I      105  69 151   i\n 74  4a 112   J      106  6a 152   j\n 75  4b 113   K      107  6b 153   k\n 76  4c 114   L      108  6c 154   l\n 77  4d 115   M      109  6d 155   m\n 78  4e 116   N      110  6e 156   n\n 79  4f 117   O      111  6f 157   o\n 80  50 120   P      112  70 160   p\n 81  51 121   Q      113  71 161   q\n 82  52 122   R      114  72 162   r\n 83  53 123   S      115  73 163   s\n 84  54 124   T      116  74 164   t\n 85  55 125   U      117  75 165   u\n 86  56 126   V      118  76 166   v\n 87  57 127   W      119  77 167   w\n 88  58 130   X      120  78 170   x\n 89  59 131   Y      121  79 171   y\n 90  5a 132   Z      122  7a 172   z\n\n\nFor a full description of the 7-bit ascii codes in their entirety, including the extended ASCII codes (where you will find things like ö and é), see this webpage:\nhttp://www.asciitable.com (ASCII Table and Extended ASCII Codes).\nIn Python you can find the ASCII integer value of a character using the ord() function. You can get the character value of an ASCII code using the chr() function.\n\nord('A')\n\n65\n\n\n\nchr(65)\n\n'A'\n\n\nYou can use your knowledge of ASCII codes to do clever things, like convert to and from uppercase and lowercase, given your knowledge that the difference (in decimal) between ASCII A and ASCII a is 32 (see the ASCII table above):\n\nchr(ord('A')+32)\n\n'a'\n\n\n\nchr(ord('a')-32)\n\n'A'\n\n\nOf course in Python there are more straightforward ways to convert between upper and lower case:\n\n'a'.upper()\n\n'A'\n\n\n\n'A'.lower()\n\n'a'"
  },
  {
    "objectID": "digital_representation_of_data.html#unicode",
    "href": "digital_representation_of_data.html#unicode",
    "title": "Digital Representation of Data",
    "section": "Unicode",
    "text": "Unicode\nThe ASCII codes only represent a limited number of characters that are useful mostly in the English language. Starting in the 1980s, Xerox, Apple, and others began work on a new variable-length encoding scheme that could represent a much larger number of characters that would be useful for the world’s languages (and now even for emoji). This is called Unicode and includes the most common standard on the web, UTF-8, which can encode more than a million different characters and symbols.\nHere is a website where you can view and search the Unicode character table.\nFor example, in Unicode the smiling face emoji 😀 is encoded using hexadecimal value 1F600:\n\nprint(f\"Unicode (hex) 1f600 is {chr(0x1f600)}\")\n\nUnicode (hex) 1f600 is 😀"
  },
  {
    "objectID": "concepts.html",
    "href": "concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "installing our development environment"
  },
  {
    "objectID": "concepts.html#week-1",
    "href": "concepts.html#week-1",
    "title": "Concepts",
    "section": "",
    "text": "installing our development environment"
  },
  {
    "objectID": "concepts.html#week-2",
    "href": "concepts.html#week-2",
    "title": "Concepts",
    "section": "Week 2",
    "text": "Week 2\n\nediting a .py Python program and executing it in the shell\nthe iPython REPL\nuv add ipython to install; and then uv run ipython to launch\nexpressions, operators, values, & variables\noperator precedence\nusing brackets ( and ) to override operator precedence\nnumeric vs character string data types\nvariables: str, int, float, bool, inspect using type()\nconversion: str(), int(), float()\noperators: + - * /, **, modulus %\nlogical operators &lt; &gt; == &gt;= &lt;= != and or not True False\nimport statements—for example, from math import cos\nprint() and formatted output using f-strings\ngetting input from the user using input()\ncommenting code using #\nreserved keywords in Python (e.g. for, return, etc)\ngetting help using help()\nVisual Studio Code"
  },
  {
    "objectID": "concepts.html#week-3",
    "href": "concepts.html#week-3",
    "title": "Concepts",
    "section": "Week 3",
    "text": "Week 3\n\nconditionals if elif else\nloops using while and for\nrange() versus list types\n\n“range objects are a specific type of sequence in Python, they behave similarly to lists or tuples but are immutable and lazy”\n\nthe FizzBuzz coding exercise\nzero-based indexing in Python\nputting it all together: testing primeness of integers"
  },
  {
    "objectID": "concepts.html#week-4",
    "href": "concepts.html#week-4",
    "title": "Concepts",
    "section": "Week 4",
    "text": "Week 4\n\nlists in Python\nlist comprehensions e.g. five_odd_numbers = [i for i in range(1,11,2)]\nin Python list variables are pointers\nb = a vs b = a.copy()\nb == a vs b is a\nother complex data types in Python: tuples, ranges, dictionaries, and sets\nwriting your own functions in Python\n\ndefining a function\nfunction header\ninput arguments\nthe work\nreturning output(s)\nvariable scope\nnamed inputs\ndefault values\n\nthe idea of modularity of code"
  },
  {
    "objectID": "concepts.html#week-5",
    "href": "concepts.html#week-5",
    "title": "Concepts",
    "section": "Week 5",
    "text": "Week 5\n\nNumPy arrays\ncreating arrays\nnp.zeros() and np.ones()\nshape of arrays using np.shape()\nmultidimensional arrays\nvectorized operations on arrays\nslicing & indexing into arrays\nfile input & output: low-level\n\nall files are binary, a series of 8-bit bytes\nASCII encoding (and utf-8 more modern version)\nusing hex editor to view a file as hexadecimal bytes\nPython defaults: 32-bit int, 64-bit float, 8-bit char\nlittle-endian vs big-endian byte ordering (sys.byteorder to check)\nusing numpy to read and write binary bytes by specifying dtype\n\nfile input & output: high-level using NumPy & pandas"
  },
  {
    "objectID": "concepts.html#week-6",
    "href": "concepts.html#week-6",
    "title": "Concepts",
    "section": "Week 6",
    "text": "Week 6\n\nprocedural programming: functions acting on data structures\nobject-oriented programming (OOP): classes & objects encapsulate attributes & methods\nhierarchical organization of Classes (and superclasses and subclasses)\nattributes (data)\nmethods (functions)\nthe __init__() method and the self variable\ninheritance\noverriding inherited methods\npolymorphism\nsuper().__init__()\nspecial methods: __str__() and __repr__()\ncopying objects\n\nshallow copy using the copy module copy.copy()\ndeep copy using copy.deepcopy()\n\noperator overloading, e.g. __eq()__, __lt__(), __gt__()"
  },
  {
    "objectID": "clean_code.html",
    "href": "clean_code.html",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "Writing small scripts to solve toy programming puzzles is one thing, but writing a large amount of inter-connected code to analyse a dataset is quite another. What’s more, organizing the data itself is something that can have a big impact on the organization and clarity of the code. It’s worth talking about and thinking about some basic principles for organizing data and code for realistic scenarios such as scientific experiments and data analysis work.\nBefore we get into principles, here is a cautionary tale.\nA Scientist’s Nightmare: Software Problem Leads to Five Retractions\nIn 2007, Geoffrey Chang, a rising star in structural biology, had to retract five papers (three from Science, one from PNAS and one from J Mol Biol). The reason? A bug in a data-processing script had flipped two columns of data, inverting the electron-density map from which his team had derived protein structures. The code ran without errors. The results looked plausible. The bug hid in plain sight for years because the code was written in a way that made it very difficult for anyone, including Chang himself, to verify what it was actually doing.\nThis is not an isolated incident. Researchers in genomics, economics, and psychology have all experienced retractions or corrections traced back to code errors that might have been caught earlier if the code had been written more carefully.\nThe point is: your analysis code is a methods section. If nobody can read it, nobody can reproduce your results, and nobody, including future-you, can catch mistakes. Clean code is not about aesthetics. It is about scientific integrity.\nI want you to read the following:\n\nWriting clean code guide from the Jörn Diedrichsen lab blog\n\nThis guide is excellent and covers a lot of ground. What follows below expands on several of its key ideas with worked examples in Python, using the kinds of data and analyses you’ve been working with this semester.\n\n\n\nThe single easiest thing you can do to improve your code is to name things well (variables, functions, files, directories). Good names make comments unnecessary. Bad names make even simple code confusing.\n\n\nConsider this snippet:\nd = np.loadtxt(\"data.csv\", delimiter=\",\", skiprows=1)\nx = d[:, 2]\nm = np.mean(x[x &lt; 1.5])\nIt runs. It produces a number. But what is d? What is column 2? What does 1.5 mean? Now compare:\ngrip_data = np.loadtxt(\"grip_strength_data.csv\", delimiter=\",\", skiprows=1)\npost_grip_force = grip_data[:, 2]\nmax_force_cutoff = 1.5  # exclude trials above 1.5 N/kg (equipment ceiling)\nmean_force = np.mean(post_grip_force[post_grip_force &lt; max_force_cutoff])\nSame computation. But now the code tells you what it is doing. A reader does not need to cross-reference a data dictionary to figure out what column 2 contains.\nSome guidelines:\n\nUse descriptive nouns for variables: reaction_times, participant_ids, mean_rt_caffeine. Not d, x, m.\nBe consistent: if you abbreviate “reaction time” as rt, use rt everywhere. Don’t mix rt, react_time, RT, and reaction_t in the same project.\nSingle-letter names are fine in small scopes: for i in range(n_trials) is perfectly clear. A loop index does not need to be called trial_index unless the loop body is very long.\nName magic numbers: if 200 appears in a line of code, nobody knows why. min_rt_ms = 200 makes the intent obvious.\n\n\n\n\nWhere variable names are nouns, function names should be verbs (or verb phrases). They should say what the function does:\n\n\n\nBad\nBetter\n\n\n\n\nfunc1()\nload_participant_data()\n\n\nprocess()\nexclude_outlier_trials()\n\n\ndo_stats()\nrun_paired_ttest()\n\n\nmy_plot()\nplot_condition_means()\n\n\n\n\n\n\nThe same principle applies to file names:\n\n\n\nBad\nBetter\n\n\n\n\ndata.csv\ngrip_strength_raw.csv\n\n\nanalysis.py\n01_preprocess.py or analyze_grip_strength.py\n\n\nfig1.png\nfigure_grip_by_condition.png\n\n\nUntitled3.py\n(delete it) 😜\n\n\n\n\n\n\n\nWhen you first start writing analysis code, the natural thing to do is to write a script: a long sequence of steps, from loading data to producing a figure, all in one file. This works fine for short tasks. But as your analysis grows, a single long script becomes hard to read, hard to debug, and hard to reuse.\nThe remedy is to modularize your code by breaking it up into functions, each of which does one specific thing.\n\n\nHere is a working script that analyses a small grip strength experiment. It loads data, excludes outlier trials, computes condition means for each participant, runs a paired t-test, and makes a bar chart. The code is realistic, and it produces the correct result. But it is hard to read. Go through it line by line with an eye towards trying to understand what it does (and remember, this could be your code, being read by future-you):\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nd = np.loadtxt(\"grip_strength_data.csv\", delimiter=\",\", skiprows=1)\n# col 0 = subj, col 1 = condition (0=placebo,1=caffeine), col 2 = force\ns = np.unique(d[:, 0])\nm1 = []\nm2 = []\nfor i in s:\n    tmp = d[d[:, 0] == i]\n    tmp1 = tmp[tmp[:, 1] == 0]\n    vals1 = tmp1[:, 2]\n    vals1 = vals1[(vals1 &gt; 5) & (vals1 &lt; 80)]\n    m1.append(np.mean(vals1))\n    tmp2 = tmp[tmp[:, 1] == 1]\n    vals2 = tmp2[:, 2]\n    vals2 = vals2[(vals2 &gt; 5) & (vals2 &lt; 80)]\n    m2.append(np.mean(vals2))\nm1 = np.array(m1)\nm2 = np.array(m2)\nt, p = stats.ttest_rel(m2, m1)\nprint(f\"t = {t:.3f}, p = {p:.3f}\")\nfig, ax = plt.subplots()\nmeans = [np.mean(m1), np.mean(m2)]\nsems = [stats.sem(m1), stats.sem(m2)]\nax.bar([0, 1], means, yerr=sems, capsize=5, color=[\"#4e79a7\", \"#e15759\"])\nax.set_xticks([0, 1])\nax.set_xticklabels([\"Placebo\", \"Caffeine\"])\nax.set_ylabel(\"Grip Force (N)\")\nax.set_title(\"Grip Strength by Condition\")\nplt.tight_layout()\nplt.savefig(\"fig.png\", dpi=150)\nplt.show()\nWhat’s wrong with this code? It works, but:\n\nVariable names like d, s, m1, m2, tmp, tmp1, tmp2, vals1, vals2 say nothing about what they contain\nThe magic numbers 5 and 80 appear without explanation\nThe block that computes means for placebo is nearly identical to the block for caffeine (copy-paste code, a classic no-no)\nThere are no functions, so nothing is reusable or independently testable\nIt is 30+ lines with no structure; to understand the flow you must read every line\n\n\n\n\nHere is the same analysis, refactored. Read through it with an eye towards understanding what is happening and how it works:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\nMIN_FORCE_N =  5   # below this = no grip detected\nMAX_FORCE_N = 80   # above this = equipment ceiling\n\nCONDITION_PLACEBO  = 0\nCONDITION_CAFFEINE = 1\n\n\ndef load_grip_data(filepath):\n    \"\"\"Load grip strength data from a CSV file.\n\n    Args:\n        filepath: Path to CSV with columns [subject_id, condition, force_n].\n\n    Returns:\n        NumPy array with shape (n_trials, 3).\n    \"\"\"\n    return np.loadtxt(filepath, delimiter=\",\", skiprows=1)\n\n\ndef compute_subject_means(data, condition):\n    \"\"\"Compute mean grip force per subject for a given condition.\n\n    Trials with force outside (MIN_FORCE_N, MAX_FORCE_N) are excluded.\n\n    Args:\n        data: Array with columns [subject_id, condition, force_n].\n        condition: Condition code to select (0 or 1).\n\n    Returns:\n        Array of per-subject mean forces.\n    \"\"\"\n    subject_ids = np.unique(data[:, 0])\n    subject_means = np.zeros(len(subject_ids))\n    for i, subj in enumerate(subject_ids):\n        subj_trials = data[(data[:, 0] == subj) & (data[:, 1] == condition)]\n        forces = subj_trials[:, 2]\n        valid = forces[(forces &gt; MIN_FORCE_N) & (forces &lt; MAX_FORCE_N)]\n        subject_means[i] = np.mean(valid)\n    return subject_means\n\n\ndef plot_condition_means(means_placebo, means_caffeine, output_path):\n    \"\"\"Create a bar chart comparing grip force between conditions.\n\n    Args:\n        means_placebo: Array of per-subject means for placebo.\n        means_caffeine: Array of per-subject means for caffeine.\n        output_path: Filepath for the saved figure.\n    \"\"\"\n    group_means = [np.mean(means_placebo), np.mean(means_caffeine)]\n    group_sems = [stats.sem(means_placebo), stats.sem(means_caffeine)]\n\n    fig, ax = plt.subplots()\n    ax.bar([0, 1], group_means, yerr=group_sems, capsize=5,\n           color=[\"#4e79a7\", \"#e15759\"])\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Placebo\", \"Caffeine\"])\n    ax.set_ylabel(\"Grip Force (N)\")\n    ax.set_title(\"Grip Strength by Condition\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150)\n    plt.show()\n\n\n# --- Main analysis ---\nif __name__ == \"__main__\":\n    grip_data = load_grip_data(\"grip_strength_data.csv\")\n\n    means_placebo  = compute_subject_means(grip_data, CONDITION_PLACEBO)\n    means_caffeine = compute_subject_means(grip_data, CONDITION_CAFFEINE)\n\n    t_stat, p_value = stats.ttest_rel(means_caffeine, means_placebo)\n    print(f\"Paired t-test: t = {t_stat:.3f}, p = {p_value:.3f}\")\n\n    plot_condition_means(means_placebo, means_caffeine, \"figure_grip_by_condition.png\")\nWhat changed?\nThe main thing is that the overall structure of the python script changed so that we have variables with important values defined at the top, then a series of functions defined in the body, and at the bottom the if __name__ == \"__main__\": “main” section defined.\nRecall when a Python script has a if __name__ == \"__main__\": section defined, that code is executed when the script is executed (e.g. from a terminal with python3 myscript.py). When the script is imported by other python code (e.g. import myscript) then the variables and functions are imported but the “main” part is not executed.\nOther things that are different in the refactored code:\n\nMagic numbers are named constants at the top of the file. If the cutoff changes, you change one line, not two (or four, or six).\nThe copy-pasted blocks are now one function (compute_subject_means) called twice with different arguments. If you need to change the outlier logic, you change it in one place.\nEach function has a docstring that says what it does, what it takes, and what it returns.\nThe main analysis reads like an outline: load data, compute means, run test, make figure. You can understand the flow without reading the details of each step.\nEach function is independently testable: you could call compute_subject_means on a small hand-crafted array and verify it gives the right answer.\n\nThe code is longer, but it is not more complex. In fact it is less complex because the structure is explicit rather than implicit.\n\n\n\n\nFunctions should do one thing, and do it well.\nFunctions should only receive the information they need\nYour top-level script should read like a table of contents: load, process, analyze, plot.\nAim for shallow call depth: ideally you have elemental functions (do one operation), process functions (chain several elemental functions), and scripts (call process functions). Three levels is usually enough.\n\n\n\n\n\nThere is a saying (I think from the Code Complete book by Steve McConnell) that good code is its own best documentation.\nIf you have to write very long comments to explain how your code works, it may be because your code is not very clean. Ideally, code should be self-explanatory.\n\n\nGood comments explain why, not what:\n# Exclude the first 5 trials per block (practice trials, not analyzed)\nvalid_trials = all_trials[all_trials[\"trial_num\"] &gt; 5]\nThe code already says what it does (select trials with trial_num &gt; 5). The comment says why (they are practice trials).\n\n\n\n# compute the mean\nm = np.mean(x)\nThis comment adds nothing. The code is already clear (well, it would be clearer if x had a better name).\n\n\n\nEvery function you write for a project should have a docstring. It doesn’t need to be long. A good convention to follow is Google-style (Google Python Style Guide: Functions and Methods).\nAt a minimum a good docstring should contain:\n\na short description of what the function does\nArgs: what the input arguments are and default values if they exist\nReturns: what the output argument is\n\ndef exclude_outlier_trials(reaction_times, min_ms=200, max_ms=2000):\n    \"\"\"Remove trials with reaction times outside a valid range.\n\n    Args:\n        reaction_times: 1D array of RTs in milliseconds.\n        min_ms: Lower cutoff (default 200 ms).\n        max_ms: Upper cutoff (default 2000 ms).\n\n    Returns:\n        1D array with outlier trials removed.\n    \"\"\"\n    return reaction_times[(reaction_times &gt;= min_ms) & (reaction_times &lt;= max_ms)]\nNotice that with a descriptive function name and a good docstring, the function body barely needs any inline comments.\n\n\n\n\n\n\nIt is useful to think about the life cycle of data as we move from “raw” data files all the way to publication-quality figures and statistical tests.\nIn data oriented scientific research we often start with one or more “raw” data files. These can be generated by pieces of laboratory equipment (e.g. EEG, fMRI, EMG, RTs from a keyboard, participant responses, etc) or they may be provided by others.\nTypically there is a need to process, or filter, or slice, or perform other operations on raw data before we reach the stage where we can perform statistical analyses or plot summary figures. It is useful to think about a first stage of data processing whereby we load in raw data files and generate “processed” or “analysed” data files that store a more useful component or product of the raw data files. Often the analysed data files are much smaller (e.g. orders of magnitude smaller) than the raw data files. For example we may have 100 raw data files, one for each participant, and after filtering, slicing, and separating out the most important components, we may end up with 100 analysed data files, one for each participant, that are each 10x smaller than the raw data files.\nThen there may be one or more kinds of analyses, and/or plots, that one wishes to perform, and instead of loading raw data files from scratch and performing the filtering/processing steps above, again, instead we operate directly on the analysed data files. This saves time (faster to load) and it also separates out the first stage of data processing from the second stage of summary analyses, plots, etc.\nThen often we can think of a third step, whereby we want to combine or summarize the analysed data files (e.g. one per participant) into group-level analyses. Perhaps from each participant’s analysed data file, we extract key dependent measures, and collect them together, for all participants, into a single tabular file (e.g. a .csv or .tsv), what we might call a summary file. Then all statistical analyses, group-level plots, can operate directly from the summary files, without having to load the analysed files or the raw files.\nOne great advantage of thinking about the workflow in this way is that each stage of data analysis is kept perfectly modular, and only depends upon the data files a the level “below”. You can pass summary data to a colleague and only need to give them the code a the top level, which operates on the summary data. Code and data organized together this way is extremely portable, understandable, debuggable, and shareable.\nIn your own research, think about the stages of processing between raw data, intermediate data structures, and “final” data used for figures and statistical analyses.\n\nfrom the Writing clean code guide by Jörn Diedrichsen; go and read the section in his blog post titled “Data Structures”\nIt should be crystal clear how to get each Figure or statistical analysis from data + code. Jörn suggests it and I agree: write out on a piece of paper what this directed graph looks like for your paper/project.\n\n\n\nHere is a sensible layout for a small research project:\ngrip_strength_study/\n├── README.md\n├── requirements.txt\n├── data/\n│   └── grip_strength_raw.csv\n├── scripts/\n│   ├── 01_preprocess.py\n│   └── 02_analyze.py\n├── figures/\n│   └── figure_grip_by_condition.png\n└── results/\n    └── stats_output.txt\nA few principles:\n\nRaw data is read-only. Never modify your original data files. If you need to preprocess, write a script that reads the raw data and writes processed data to a separate file or directory.\nSeparate code from data from outputs. This makes it obvious what is source material, what is code, and what is generated.\nInclude a README.md that says what the project is, how to run the analysis, and what the dependencies are.\nInclude a requirements.txt (or pyproject.toml) listing your Python dependencies and their version numbers, so someone else can recreate your environment. If you are using uv, a pyproject.toml file will be created for you.\n\nIf you end up with a large number of data files and distinct “raw”, “processed” and “summary” levels of data structures, put them in distinct directories, e.g. data/raw/, data/processed/ and data/summary/.\n\n\n\n\nCode should reside in a publicly accessible repository (e.g. on GitHub)\nEach Figure and each statistical analysis should be reproducible by running a single script/function without changes\nData should accompany the code (GitHub is not the place to store very large data files, but files at the summary level, e.g. .csv tabular files are likely fine)\n\n\n\n\nGitHub is a version control system and code-sharing platform that allows you to track changes to your code over time and collaborate with others. You can use GitHub to store and manage your code, data analysis notebooks, and even data (though not gigantic amounts of data), ensuring you can revert to previous versions if needed. GitHub works well with a number of other useful programs like Visual Studio Code and the online LaTeX document system Overleaf.\n\nGet Started with GitHub\nGitHub Skills\n\n\n\n\n\n\n\nDownload the clean-code-exercises.tgz archive, move it to your Psych_9040/ directory, and unpack it:\n$ tar -xvzf clean-code-exercises.tgz\nThis is what it looks like:\n$ tree -F clean-code-exercises\nclean-code-exercises/\n├── exercise1/\n│   ├── messy_analysis.py\n│   ├── data/\n│   │   └── rt_experiment_data.csv\n│   └── TASKS.md\n├── exercise2/\n│   ├── analyze_data.py\n│   ├── experiment_data.csv\n│   ├── make_figure.py\n│   ├── test.py\n│   ├── Untitled1.py\n│   ├── fig1.png\n│   └── TASKS.md\n└── README.md\n\n\n\nRefactor a messy script.\n\n\nIn the exercise1/ directory you will find a script called messy_analysis.py and a data file data/rt_experiment_data.csv. The data file contains simulated reaction time data from a visual search experiment with two conditions (feature search and conjunction search). The script loads the data, excludes outlier trials, computes per-subject condition means, runs a paired t-test, and makes a bar chart.\nThe script works. Run it and confirm you get a t-statistic, a p-value, and a figure.\nYour task is to refactor the script so that:\n\nVariable names are descriptive\nMagic numbers are replaced with named constants\nRepeated code is eliminated using functions\nEach function has a Google-style docstring\nThe main analysis reads like a short outline\n\nYour refactored script must produce the same numerical output and the same figure.\nSubmit: your refactored .py file.\n\n\n\n\nHow many functions did you end up with?\nWhich parts of the original script were hardest to clean up?\nCould someone unfamiliar with the experiment understand your refactored code without additional explanation?\n\n\n\n\n\nOrganize a messy project.\n\n\nThe exercise2/ directory contains a small “project” — a handful of scripts, a data file, and an output figure, all dumped flat in one directory. Some files are useful. Some are not. There is no README.\nExamine the files and figure out what the project does. Then:\n\nReorganize the project into a clean directory structure (e.g. data/, scripts/, figures/).\nDelete files that are clearly junk (scratch files, unnamed files).\nRename files to be descriptive.\nWrite a README.md that describes: what this project does, what the data file contains, how to run the analysis, and what packages are needed.\nCreate a requirements.txt listing the Python dependencies.\n(Optional) Initialize a git repository (git init), add your files, and make a first commit.\n\nSubmit: your reorganized project directory (as a .zip or .tgz), including the README.\n\n\n\n\nWhat directory structure did you choose and why?\nWhat did you put in the README?\nIf you came back to this project in a year, would you know what to do?\n\n\n\n\n\n\n\nWriting clean code guide from Jörn Diedrichsen’s lab\nBetter Code, Better Science by Russ Poldrack — especially the Principles of software engineering and Project structure and management chapters\nbook: Clean Code: A Handbook of Agile Software Craftsmanship by Robert Martin\na shorter book: A philosophy of software design by John Ousterhout, (and an accompanying website)\nPEP 8 — Python’s official style guide"
  },
  {
    "objectID": "clean_code.html#why",
    "href": "clean_code.html#why",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "Writing small scripts to solve toy programming puzzles is one thing, but writing a large amount of inter-connected code to analyse a dataset is quite another. What’s more, organizing the data itself is something that can have a big impact on the organization and clarity of the code. It’s worth talking about and thinking about some basic principles for organizing data and code for realistic scenarios such as scientific experiments and data analysis work.\nBefore we get into principles, here is a cautionary tale.\nA Scientist’s Nightmare: Software Problem Leads to Five Retractions\nIn 2007, Geoffrey Chang, a rising star in structural biology, had to retract five papers (three from Science, one from PNAS and one from J Mol Biol). The reason? A bug in a data-processing script had flipped two columns of data, inverting the electron-density map from which his team had derived protein structures. The code ran without errors. The results looked plausible. The bug hid in plain sight for years because the code was written in a way that made it very difficult for anyone, including Chang himself, to verify what it was actually doing.\nThis is not an isolated incident. Researchers in genomics, economics, and psychology have all experienced retractions or corrections traced back to code errors that might have been caught earlier if the code had been written more carefully.\nThe point is: your analysis code is a methods section. If nobody can read it, nobody can reproduce your results, and nobody, including future-you, can catch mistakes. Clean code is not about aesthetics. It is about scientific integrity.\nI want you to read the following:\n\nWriting clean code guide from the Jörn Diedrichsen lab blog\n\nThis guide is excellent and covers a lot of ground. What follows below expands on several of its key ideas with worked examples in Python, using the kinds of data and analyses you’ve been working with this semester."
  },
  {
    "objectID": "clean_code.html#naming",
    "href": "clean_code.html#naming",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "The single easiest thing you can do to improve your code is to name things well (variables, functions, files, directories). Good names make comments unnecessary. Bad names make even simple code confusing.\n\n\nConsider this snippet:\nd = np.loadtxt(\"data.csv\", delimiter=\",\", skiprows=1)\nx = d[:, 2]\nm = np.mean(x[x &lt; 1.5])\nIt runs. It produces a number. But what is d? What is column 2? What does 1.5 mean? Now compare:\ngrip_data = np.loadtxt(\"grip_strength_data.csv\", delimiter=\",\", skiprows=1)\npost_grip_force = grip_data[:, 2]\nmax_force_cutoff = 1.5  # exclude trials above 1.5 N/kg (equipment ceiling)\nmean_force = np.mean(post_grip_force[post_grip_force &lt; max_force_cutoff])\nSame computation. But now the code tells you what it is doing. A reader does not need to cross-reference a data dictionary to figure out what column 2 contains.\nSome guidelines:\n\nUse descriptive nouns for variables: reaction_times, participant_ids, mean_rt_caffeine. Not d, x, m.\nBe consistent: if you abbreviate “reaction time” as rt, use rt everywhere. Don’t mix rt, react_time, RT, and reaction_t in the same project.\nSingle-letter names are fine in small scopes: for i in range(n_trials) is perfectly clear. A loop index does not need to be called trial_index unless the loop body is very long.\nName magic numbers: if 200 appears in a line of code, nobody knows why. min_rt_ms = 200 makes the intent obvious.\n\n\n\n\nWhere variable names are nouns, function names should be verbs (or verb phrases). They should say what the function does:\n\n\n\nBad\nBetter\n\n\n\n\nfunc1()\nload_participant_data()\n\n\nprocess()\nexclude_outlier_trials()\n\n\ndo_stats()\nrun_paired_ttest()\n\n\nmy_plot()\nplot_condition_means()\n\n\n\n\n\n\nThe same principle applies to file names:\n\n\n\nBad\nBetter\n\n\n\n\ndata.csv\ngrip_strength_raw.csv\n\n\nanalysis.py\n01_preprocess.py or analyze_grip_strength.py\n\n\nfig1.png\nfigure_grip_by_condition.png\n\n\nUntitled3.py\n(delete it) 😜"
  },
  {
    "objectID": "clean_code.html#modularity",
    "href": "clean_code.html#modularity",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "When you first start writing analysis code, the natural thing to do is to write a script: a long sequence of steps, from loading data to producing a figure, all in one file. This works fine for short tasks. But as your analysis grows, a single long script becomes hard to read, hard to debug, and hard to reuse.\nThe remedy is to modularize your code by breaking it up into functions, each of which does one specific thing.\n\n\nHere is a working script that analyses a small grip strength experiment. It loads data, excludes outlier trials, computes condition means for each participant, runs a paired t-test, and makes a bar chart. The code is realistic, and it produces the correct result. But it is hard to read. Go through it line by line with an eye towards trying to understand what it does (and remember, this could be your code, being read by future-you):\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nd = np.loadtxt(\"grip_strength_data.csv\", delimiter=\",\", skiprows=1)\n# col 0 = subj, col 1 = condition (0=placebo,1=caffeine), col 2 = force\ns = np.unique(d[:, 0])\nm1 = []\nm2 = []\nfor i in s:\n    tmp = d[d[:, 0] == i]\n    tmp1 = tmp[tmp[:, 1] == 0]\n    vals1 = tmp1[:, 2]\n    vals1 = vals1[(vals1 &gt; 5) & (vals1 &lt; 80)]\n    m1.append(np.mean(vals1))\n    tmp2 = tmp[tmp[:, 1] == 1]\n    vals2 = tmp2[:, 2]\n    vals2 = vals2[(vals2 &gt; 5) & (vals2 &lt; 80)]\n    m2.append(np.mean(vals2))\nm1 = np.array(m1)\nm2 = np.array(m2)\nt, p = stats.ttest_rel(m2, m1)\nprint(f\"t = {t:.3f}, p = {p:.3f}\")\nfig, ax = plt.subplots()\nmeans = [np.mean(m1), np.mean(m2)]\nsems = [stats.sem(m1), stats.sem(m2)]\nax.bar([0, 1], means, yerr=sems, capsize=5, color=[\"#4e79a7\", \"#e15759\"])\nax.set_xticks([0, 1])\nax.set_xticklabels([\"Placebo\", \"Caffeine\"])\nax.set_ylabel(\"Grip Force (N)\")\nax.set_title(\"Grip Strength by Condition\")\nplt.tight_layout()\nplt.savefig(\"fig.png\", dpi=150)\nplt.show()\nWhat’s wrong with this code? It works, but:\n\nVariable names like d, s, m1, m2, tmp, tmp1, tmp2, vals1, vals2 say nothing about what they contain\nThe magic numbers 5 and 80 appear without explanation\nThe block that computes means for placebo is nearly identical to the block for caffeine (copy-paste code, a classic no-no)\nThere are no functions, so nothing is reusable or independently testable\nIt is 30+ lines with no structure; to understand the flow you must read every line\n\n\n\n\nHere is the same analysis, refactored. Read through it with an eye towards understanding what is happening and how it works:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\nMIN_FORCE_N =  5   # below this = no grip detected\nMAX_FORCE_N = 80   # above this = equipment ceiling\n\nCONDITION_PLACEBO  = 0\nCONDITION_CAFFEINE = 1\n\n\ndef load_grip_data(filepath):\n    \"\"\"Load grip strength data from a CSV file.\n\n    Args:\n        filepath: Path to CSV with columns [subject_id, condition, force_n].\n\n    Returns:\n        NumPy array with shape (n_trials, 3).\n    \"\"\"\n    return np.loadtxt(filepath, delimiter=\",\", skiprows=1)\n\n\ndef compute_subject_means(data, condition):\n    \"\"\"Compute mean grip force per subject for a given condition.\n\n    Trials with force outside (MIN_FORCE_N, MAX_FORCE_N) are excluded.\n\n    Args:\n        data: Array with columns [subject_id, condition, force_n].\n        condition: Condition code to select (0 or 1).\n\n    Returns:\n        Array of per-subject mean forces.\n    \"\"\"\n    subject_ids = np.unique(data[:, 0])\n    subject_means = np.zeros(len(subject_ids))\n    for i, subj in enumerate(subject_ids):\n        subj_trials = data[(data[:, 0] == subj) & (data[:, 1] == condition)]\n        forces = subj_trials[:, 2]\n        valid = forces[(forces &gt; MIN_FORCE_N) & (forces &lt; MAX_FORCE_N)]\n        subject_means[i] = np.mean(valid)\n    return subject_means\n\n\ndef plot_condition_means(means_placebo, means_caffeine, output_path):\n    \"\"\"Create a bar chart comparing grip force between conditions.\n\n    Args:\n        means_placebo: Array of per-subject means for placebo.\n        means_caffeine: Array of per-subject means for caffeine.\n        output_path: Filepath for the saved figure.\n    \"\"\"\n    group_means = [np.mean(means_placebo), np.mean(means_caffeine)]\n    group_sems = [stats.sem(means_placebo), stats.sem(means_caffeine)]\n\n    fig, ax = plt.subplots()\n    ax.bar([0, 1], group_means, yerr=group_sems, capsize=5,\n           color=[\"#4e79a7\", \"#e15759\"])\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Placebo\", \"Caffeine\"])\n    ax.set_ylabel(\"Grip Force (N)\")\n    ax.set_title(\"Grip Strength by Condition\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150)\n    plt.show()\n\n\n# --- Main analysis ---\nif __name__ == \"__main__\":\n    grip_data = load_grip_data(\"grip_strength_data.csv\")\n\n    means_placebo  = compute_subject_means(grip_data, CONDITION_PLACEBO)\n    means_caffeine = compute_subject_means(grip_data, CONDITION_CAFFEINE)\n\n    t_stat, p_value = stats.ttest_rel(means_caffeine, means_placebo)\n    print(f\"Paired t-test: t = {t_stat:.3f}, p = {p_value:.3f}\")\n\n    plot_condition_means(means_placebo, means_caffeine, \"figure_grip_by_condition.png\")\nWhat changed?\nThe main thing is that the overall structure of the python script changed so that we have variables with important values defined at the top, then a series of functions defined in the body, and at the bottom the if __name__ == \"__main__\": “main” section defined.\nRecall when a Python script has a if __name__ == \"__main__\": section defined, that code is executed when the script is executed (e.g. from a terminal with python3 myscript.py). When the script is imported by other python code (e.g. import myscript) then the variables and functions are imported but the “main” part is not executed.\nOther things that are different in the refactored code:\n\nMagic numbers are named constants at the top of the file. If the cutoff changes, you change one line, not two (or four, or six).\nThe copy-pasted blocks are now one function (compute_subject_means) called twice with different arguments. If you need to change the outlier logic, you change it in one place.\nEach function has a docstring that says what it does, what it takes, and what it returns.\nThe main analysis reads like an outline: load data, compute means, run test, make figure. You can understand the flow without reading the details of each step.\nEach function is independently testable: you could call compute_subject_means on a small hand-crafted array and verify it gives the right answer.\n\nThe code is longer, but it is not more complex. In fact it is less complex because the structure is explicit rather than implicit.\n\n\n\n\nFunctions should do one thing, and do it well.\nFunctions should only receive the information they need\nYour top-level script should read like a table of contents: load, process, analyze, plot.\nAim for shallow call depth: ideally you have elemental functions (do one operation), process functions (chain several elemental functions), and scripts (call process functions). Three levels is usually enough."
  },
  {
    "objectID": "clean_code.html#comments-docstrings",
    "href": "clean_code.html#comments-docstrings",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "There is a saying (I think from the Code Complete book by Steve McConnell) that good code is its own best documentation.\nIf you have to write very long comments to explain how your code works, it may be because your code is not very clean. Ideally, code should be self-explanatory.\n\n\nGood comments explain why, not what:\n# Exclude the first 5 trials per block (practice trials, not analyzed)\nvalid_trials = all_trials[all_trials[\"trial_num\"] &gt; 5]\nThe code already says what it does (select trials with trial_num &gt; 5). The comment says why (they are practice trials).\n\n\n\n# compute the mean\nm = np.mean(x)\nThis comment adds nothing. The code is already clear (well, it would be clearer if x had a better name).\n\n\n\nEvery function you write for a project should have a docstring. It doesn’t need to be long. A good convention to follow is Google-style (Google Python Style Guide: Functions and Methods).\nAt a minimum a good docstring should contain:\n\na short description of what the function does\nArgs: what the input arguments are and default values if they exist\nReturns: what the output argument is\n\ndef exclude_outlier_trials(reaction_times, min_ms=200, max_ms=2000):\n    \"\"\"Remove trials with reaction times outside a valid range.\n\n    Args:\n        reaction_times: 1D array of RTs in milliseconds.\n        min_ms: Lower cutoff (default 200 ms).\n        max_ms: Upper cutoff (default 2000 ms).\n\n    Returns:\n        1D array with outlier trials removed.\n    \"\"\"\n    return reaction_times[(reaction_times &gt;= min_ms) & (reaction_times &lt;= max_ms)]\nNotice that with a descriptive function name and a good docstring, the function body barely needs any inline comments."
  },
  {
    "objectID": "clean_code.html#project-organization",
    "href": "clean_code.html#project-organization",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "It is useful to think about the life cycle of data as we move from “raw” data files all the way to publication-quality figures and statistical tests.\nIn data oriented scientific research we often start with one or more “raw” data files. These can be generated by pieces of laboratory equipment (e.g. EEG, fMRI, EMG, RTs from a keyboard, participant responses, etc) or they may be provided by others.\nTypically there is a need to process, or filter, or slice, or perform other operations on raw data before we reach the stage where we can perform statistical analyses or plot summary figures. It is useful to think about a first stage of data processing whereby we load in raw data files and generate “processed” or “analysed” data files that store a more useful component or product of the raw data files. Often the analysed data files are much smaller (e.g. orders of magnitude smaller) than the raw data files. For example we may have 100 raw data files, one for each participant, and after filtering, slicing, and separating out the most important components, we may end up with 100 analysed data files, one for each participant, that are each 10x smaller than the raw data files.\nThen there may be one or more kinds of analyses, and/or plots, that one wishes to perform, and instead of loading raw data files from scratch and performing the filtering/processing steps above, again, instead we operate directly on the analysed data files. This saves time (faster to load) and it also separates out the first stage of data processing from the second stage of summary analyses, plots, etc.\nThen often we can think of a third step, whereby we want to combine or summarize the analysed data files (e.g. one per participant) into group-level analyses. Perhaps from each participant’s analysed data file, we extract key dependent measures, and collect them together, for all participants, into a single tabular file (e.g. a .csv or .tsv), what we might call a summary file. Then all statistical analyses, group-level plots, can operate directly from the summary files, without having to load the analysed files or the raw files.\nOne great advantage of thinking about the workflow in this way is that each stage of data analysis is kept perfectly modular, and only depends upon the data files a the level “below”. You can pass summary data to a colleague and only need to give them the code a the top level, which operates on the summary data. Code and data organized together this way is extremely portable, understandable, debuggable, and shareable.\nIn your own research, think about the stages of processing between raw data, intermediate data structures, and “final” data used for figures and statistical analyses.\n\nfrom the Writing clean code guide by Jörn Diedrichsen; go and read the section in his blog post titled “Data Structures”\nIt should be crystal clear how to get each Figure or statistical analysis from data + code. Jörn suggests it and I agree: write out on a piece of paper what this directed graph looks like for your paper/project.\n\n\n\nHere is a sensible layout for a small research project:\ngrip_strength_study/\n├── README.md\n├── requirements.txt\n├── data/\n│   └── grip_strength_raw.csv\n├── scripts/\n│   ├── 01_preprocess.py\n│   └── 02_analyze.py\n├── figures/\n│   └── figure_grip_by_condition.png\n└── results/\n    └── stats_output.txt\nA few principles:\n\nRaw data is read-only. Never modify your original data files. If you need to preprocess, write a script that reads the raw data and writes processed data to a separate file or directory.\nSeparate code from data from outputs. This makes it obvious what is source material, what is code, and what is generated.\nInclude a README.md that says what the project is, how to run the analysis, and what the dependencies are.\nInclude a requirements.txt (or pyproject.toml) listing your Python dependencies and their version numbers, so someone else can recreate your environment. If you are using uv, a pyproject.toml file will be created for you.\n\nIf you end up with a large number of data files and distinct “raw”, “processed” and “summary” levels of data structures, put them in distinct directories, e.g. data/raw/, data/processed/ and data/summary/.\n\n\n\n\nCode should reside in a publicly accessible repository (e.g. on GitHub)\nEach Figure and each statistical analysis should be reproducible by running a single script/function without changes\nData should accompany the code (GitHub is not the place to store very large data files, but files at the summary level, e.g. .csv tabular files are likely fine)\n\n\n\n\nGitHub is a version control system and code-sharing platform that allows you to track changes to your code over time and collaborate with others. You can use GitHub to store and manage your code, data analysis notebooks, and even data (though not gigantic amounts of data), ensuring you can revert to previous versions if needed. GitHub works well with a number of other useful programs like Visual Studio Code and the online LaTeX document system Overleaf.\n\nGet Started with GitHub\nGitHub Skills"
  },
  {
    "objectID": "clean_code.html#exercises",
    "href": "clean_code.html#exercises",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "Download the clean-code-exercises.tgz archive, move it to your Psych_9040/ directory, and unpack it:\n$ tar -xvzf clean-code-exercises.tgz\nThis is what it looks like:\n$ tree -F clean-code-exercises\nclean-code-exercises/\n├── exercise1/\n│   ├── messy_analysis.py\n│   ├── data/\n│   │   └── rt_experiment_data.csv\n│   └── TASKS.md\n├── exercise2/\n│   ├── analyze_data.py\n│   ├── experiment_data.csv\n│   ├── make_figure.py\n│   ├── test.py\n│   ├── Untitled1.py\n│   ├── fig1.png\n│   └── TASKS.md\n└── README.md\n\n\n\nRefactor a messy script.\n\n\nIn the exercise1/ directory you will find a script called messy_analysis.py and a data file data/rt_experiment_data.csv. The data file contains simulated reaction time data from a visual search experiment with two conditions (feature search and conjunction search). The script loads the data, excludes outlier trials, computes per-subject condition means, runs a paired t-test, and makes a bar chart.\nThe script works. Run it and confirm you get a t-statistic, a p-value, and a figure.\nYour task is to refactor the script so that:\n\nVariable names are descriptive\nMagic numbers are replaced with named constants\nRepeated code is eliminated using functions\nEach function has a Google-style docstring\nThe main analysis reads like a short outline\n\nYour refactored script must produce the same numerical output and the same figure.\nSubmit: your refactored .py file.\n\n\n\n\nHow many functions did you end up with?\nWhich parts of the original script were hardest to clean up?\nCould someone unfamiliar with the experiment understand your refactored code without additional explanation?\n\n\n\n\n\nOrganize a messy project.\n\n\nThe exercise2/ directory contains a small “project” — a handful of scripts, a data file, and an output figure, all dumped flat in one directory. Some files are useful. Some are not. There is no README.\nExamine the files and figure out what the project does. Then:\n\nReorganize the project into a clean directory structure (e.g. data/, scripts/, figures/).\nDelete files that are clearly junk (scratch files, unnamed files).\nRename files to be descriptive.\nWrite a README.md that describes: what this project does, what the data file contains, how to run the analysis, and what packages are needed.\nCreate a requirements.txt listing the Python dependencies.\n(Optional) Initialize a git repository (git init), add your files, and make a first commit.\n\nSubmit: your reorganized project directory (as a .zip or .tgz), including the README.\n\n\n\n\nWhat directory structure did you choose and why?\nWhat did you put in the README?\nIf you came back to this project in a year, would you know what to do?"
  },
  {
    "objectID": "clean_code.html#resources",
    "href": "clean_code.html#resources",
    "title": "Reproducibility & Replicability II: clean code",
    "section": "",
    "text": "Writing clean code guide from Jörn Diedrichsen’s lab\nBetter Code, Better Science by Russ Poldrack — especially the Principles of software engineering and Project structure and management chapters\nbook: Clean Code: A Handbook of Agile Software Craftsmanship by Robert Martin\na shorter book: A philosophy of software design by John Ousterhout, (and an accompanying website)\nPEP 8 — Python’s official style guide"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Sample Code",
    "section": "",
    "text": "fizzbuzz.py\nis_it_prime.py\np092.py\nfileio.py\nshapes.py"
  },
  {
    "objectID": "control_flow_and_complex_data_types.html",
    "href": "control_flow_and_complex_data_types.html",
    "title": "Control Flow & Complex data types",
    "section": "",
    "text": "Here we will learn above several ways to specify the flow of information as your code gets executed. We will learn about loops, which are constructs that allow you to repeat blocks of code multiple times, typically while changing the values of variables inside the repeating block. We will learn about conditionals, which allow you to execute different branches of code depending on the values of variables.\n\n\n\nLearning with Python 3 chapter 5: Conditionals\nLearning with Python 3 chapter 7: Iteration\nPython for Data Analysis chapter 3: Built-in Data Structures, Functions, and Files\nPython for Data Analysis chapter 4: NumPy Basics: Arrays and Vectorized Computation\nPython for Data Analysis chapter 5: Getting Started with pandas\nConditionals and recursion (you can ignore for now the section on recursion)\nControl Flow\n\n\n\n\nLoops are used when you have a chunk of code that you need to repeat over and over again, each time changing one (or more) parameters.\nThere are two kinds of loops: a for loop and a while loop. A for loop is used when you (or your code) know in advance of starting the loop, how many iterations to run through. A while loop is used when the number of iterations is not known in advance of starting the loop. You might use a for loop to load in a list of data files. You might use a while loop to iterate through an EEG waveform over samples (time) to search for the first value that exceeds some baseline threshold.\nHere is a simple example for the purposes of demonstration. Let’s say you want to load data from 5 files, named data1.txt, data2.txt, …, data5.txt. Let’s say each file contains a one-dimensional array of 10 values. Let’s say you want to take the average of each data file and then report the overall mean and overall variance of those values. Here’s one way to do it:\n\n\n\nd1 = load(\"data1.txt\")\nd1m = mean(d1)\nd2 = load(\"data2.txt\")\nd2m = mean(d2)\nd3 = load(\"data3.txt\")\nd3m = mean(d3)\nd4 = load(\"data4.txt\")\nd4m = mean(d4)\nd5 = load(\"data5.txt\")\nd5m = mean(d5)\n\n# report overall mean and overall variance of 5 data file means\nalldata = [d1m, d2m, d3m, d4m, d5m]\ndatamean = mean(alldata)\ndatavar = var(alldata)\nprint(f\"mean={datamean:.3f} and variance={datavar:.3f}\")\n\nYou can see that there is a lot of repetition in this code. What if we had to load data from 1000 data files? There would be a lot of copying and pasting of code chunks. This is error prone and inefficient. Instead let’s use a for loop. A for loop allows you to repeat a block of code some predetermined number of times, and includes a counter so that you know which iteration of the loop is currently running. Here is what the code above would look like if we used a for loop:\n\nnfiles = 1000\nalldata = np.zeros(1000)\nfor i in range(nfiles):\n    d = load(\"data\" + str(i) + \".txt\")\n    alldata[i] = mean(d)\ndatamean = mean(alldata)\ndatavar = var(alldata)\nprint(f\"mean={datamean:.3f} and variance={datavar:.3f}\")\n\nNow all we would need to change if we have 1000 data files (or one million) is the value of our variable nfiles=1000 or nfiles=1e6—nothing else in the code would have to change. This makes our code much more resilient against programming errors.\nYou can see a for loop begins with the keyword for followed by a name of a variable (your choice) that will keep track of which iteration of the loop is currently running. Then for word in followed by a list of values to be iterated through. Next is the block of code to be repeated. Note that in Python, blocks of code like this are denoted using indentation. Python is very fussy indeed about indentation.\n\nfor i in range(1,15,3):\n    print(f\"i={i}\")\n\ni=1\ni=4\ni=7\ni=10\ni=13\n\n\nFor loops are executed in a serial fashion, one repetition after another.\n\n\n\nThere is a second sort of loop called a while loop. This kind of loop is typically used when the number of iterations is not known in advance. A while loop keeps repeating until the value of a logical expression changes from TRUE to FALSE (changes from 1 to 0). As a little demo, here is an example of a while loop that prints out successive integers starting from 1, until they exceed a critical value, in this case 10:\n\ni = 1\nwhile (i &lt; 9):\n    print(f\"i={i}\")\n    i = i + 1\n\ni=1\ni=2\ni=3\ni=4\ni=5\ni=6\ni=7\ni=8\n\n\nThe expression that determines whether a while loop will continue repeating can be any valid Python expression that evaluates to a boolean (true or false).\nFor both for loops and while loops, there are two keywords to know about that can break you out of a loop (break) and can move you to the next iteration of the loop (continue). I tend not to use these, but some people find them useful."
  },
  {
    "objectID": "control_flow_and_complex_data_types.html#readings",
    "href": "control_flow_and_complex_data_types.html#readings",
    "title": "Control Flow & Complex data types",
    "section": "",
    "text": "Learning with Python 3 chapter 5: Conditionals\nLearning with Python 3 chapter 7: Iteration\nPython for Data Analysis chapter 3: Built-in Data Structures, Functions, and Files\nPython for Data Analysis chapter 4: NumPy Basics: Arrays and Vectorized Computation\nPython for Data Analysis chapter 5: Getting Started with pandas\nConditionals and recursion (you can ignore for now the section on recursion)\nControl Flow"
  },
  {
    "objectID": "control_flow_and_complex_data_types.html#loops",
    "href": "control_flow_and_complex_data_types.html#loops",
    "title": "Control Flow & Complex data types",
    "section": "",
    "text": "Loops are used when you have a chunk of code that you need to repeat over and over again, each time changing one (or more) parameters.\nThere are two kinds of loops: a for loop and a while loop. A for loop is used when you (or your code) know in advance of starting the loop, how many iterations to run through. A while loop is used when the number of iterations is not known in advance of starting the loop. You might use a for loop to load in a list of data files. You might use a while loop to iterate through an EEG waveform over samples (time) to search for the first value that exceeds some baseline threshold.\nHere is a simple example for the purposes of demonstration. Let’s say you want to load data from 5 files, named data1.txt, data2.txt, …, data5.txt. Let’s say each file contains a one-dimensional array of 10 values. Let’s say you want to take the average of each data file and then report the overall mean and overall variance of those values. Here’s one way to do it:\n\n\n\nd1 = load(\"data1.txt\")\nd1m = mean(d1)\nd2 = load(\"data2.txt\")\nd2m = mean(d2)\nd3 = load(\"data3.txt\")\nd3m = mean(d3)\nd4 = load(\"data4.txt\")\nd4m = mean(d4)\nd5 = load(\"data5.txt\")\nd5m = mean(d5)\n\n# report overall mean and overall variance of 5 data file means\nalldata = [d1m, d2m, d3m, d4m, d5m]\ndatamean = mean(alldata)\ndatavar = var(alldata)\nprint(f\"mean={datamean:.3f} and variance={datavar:.3f}\")\n\nYou can see that there is a lot of repetition in this code. What if we had to load data from 1000 data files? There would be a lot of copying and pasting of code chunks. This is error prone and inefficient. Instead let’s use a for loop. A for loop allows you to repeat a block of code some predetermined number of times, and includes a counter so that you know which iteration of the loop is currently running. Here is what the code above would look like if we used a for loop:\n\nnfiles = 1000\nalldata = np.zeros(1000)\nfor i in range(nfiles):\n    d = load(\"data\" + str(i) + \".txt\")\n    alldata[i] = mean(d)\ndatamean = mean(alldata)\ndatavar = var(alldata)\nprint(f\"mean={datamean:.3f} and variance={datavar:.3f}\")\n\nNow all we would need to change if we have 1000 data files (or one million) is the value of our variable nfiles=1000 or nfiles=1e6—nothing else in the code would have to change. This makes our code much more resilient against programming errors.\nYou can see a for loop begins with the keyword for followed by a name of a variable (your choice) that will keep track of which iteration of the loop is currently running. Then for word in followed by a list of values to be iterated through. Next is the block of code to be repeated. Note that in Python, blocks of code like this are denoted using indentation. Python is very fussy indeed about indentation.\n\nfor i in range(1,15,3):\n    print(f\"i={i}\")\n\ni=1\ni=4\ni=7\ni=10\ni=13\n\n\nFor loops are executed in a serial fashion, one repetition after another.\n\n\n\nThere is a second sort of loop called a while loop. This kind of loop is typically used when the number of iterations is not known in advance. A while loop keeps repeating until the value of a logical expression changes from TRUE to FALSE (changes from 1 to 0). As a little demo, here is an example of a while loop that prints out successive integers starting from 1, until they exceed a critical value, in this case 10:\n\ni = 1\nwhile (i &lt; 9):\n    print(f\"i={i}\")\n    i = i + 1\n\ni=1\ni=2\ni=3\ni=4\ni=5\ni=6\ni=7\ni=8\n\n\nThe expression that determines whether a while loop will continue repeating can be any valid Python expression that evaluates to a boolean (true or false).\nFor both for loops and while loops, there are two keywords to know about that can break you out of a loop (break) and can move you to the next iteration of the loop (continue). I tend not to use these, but some people find them useful."
  },
  {
    "objectID": "functions_file_io.html",
    "href": "functions_file_io.html",
    "title": "Functions, File input & output",
    "section": "",
    "text": "Learning with Python 3 chapter 4: Functions\nThink Python Chapter 3\nPython for Data Analysis chapter 3.2: Functions\nPython for Data Analysis chapter 3.3: Files and the Operating System\nPython for Data Analysis chapter 6: Data Loading, Storage, and File Formats"
  },
  {
    "objectID": "functions_file_io.html#readings",
    "href": "functions_file_io.html#readings",
    "title": "Functions, File input & output",
    "section": "",
    "text": "Learning with Python 3 chapter 4: Functions\nThink Python Chapter 3\nPython for Data Analysis chapter 3.2: Functions\nPython for Data Analysis chapter 3.3: Files and the Operating System\nPython for Data Analysis chapter 6: Data Loading, Storage, and File Formats"
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Git & GitHub",
    "section": "",
    "text": "First, if you don’t have a GitHub account already, go and sign up for a (free) GitHub account.\nThen, double-check that you have installed git as part of the setup instructions. Open up a terminal and type:\ngit --version\nand you should get something like:\ngit version 2.34.0\nIt doesn’t much matter if the version number isn’t identical to the one above."
  },
  {
    "objectID": "git.html#setup",
    "href": "git.html#setup",
    "title": "Git & GitHub",
    "section": "",
    "text": "First, if you don’t have a GitHub account already, go and sign up for a (free) GitHub account.\nThen, double-check that you have installed git as part of the setup instructions. Open up a terminal and type:\ngit --version\nand you should get something like:\ngit version 2.34.0\nIt doesn’t much matter if the version number isn’t identical to the one above."
  },
  {
    "objectID": "git.html#tutorials-intros",
    "href": "git.html#tutorials-intros",
    "title": "Git & GitHub",
    "section": "Tutorials / Intros",
    "text": "Tutorials / Intros\n\nabout GitHub and Git by GitHub docs\ngit-novice by software carpentry\nhello world by GitHub docs\nGit Cheat Sheet by GitHub Education\nGit project page\nVersion Control and Git by Kieran Healy\nThe Version Control Book: Track, organize and share your work: An introduction to Git for research"
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "A Note about Homeworks",
    "section": "",
    "text": "The reason I assign homework to you each week is not because I want to see the code that you write.\nThe purpose of homework is for you to do the intellectual work required to write the code.\nIf you choose to use an AI tool (e.g. ChatGPT) to write your homework code then you will probably get a very good grade in this course but you will not have done the intellectual work.\nIn my experience (more than three decades in a university environment), reading other people’s code (or code that an AI tool wrote) is not a substitute for doing the intellectual work required to produce your own code.\nIt simply isn’t.\nIf you take this course, and you get a good grade, your supervisors / bosses / collaborators will expect you to have mastered the material and skills that are advertised in the (public) course outline, and that are reflected in the homework assignments that you hand in.\nYour grade in this course is not the goal. The intellectual work is the goal. If you don’t want to do the intellectual work, don’t take the course. It would be a waste of your time and mine.\nIf you are having difficulty with the work, come and see me or the TA, we will be absolutely pleased to help!\n—P"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Due: Jan 25 by 11:59 pm eastern standard time\nSubmit a single file called name_02.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_02.py\n\nWrite a Python program to complete the Nth Prime number exercise.\nMake sure it produces the correct output given the example inputs."
  },
  {
    "objectID": "hw/hw04.html",
    "href": "hw/hw04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Due: Feb 8 by 11:59 pm eastern standard time\nSubmit a single file called name_04.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_04.py\n\nChoose any one of the “Slighly more challenging problems” listed in the Advent of Code coding exercises. Write a Python script to complete both part 1 and part 2 of the problem you choose. Submit only your code, you don’t need to submit your puzzle input.\nIf you are already Python experienced and you want to try one of the “more challenging problems” instead, that would be fine as well.\nIf you have never programmed anything before this course and you wish to choose one of the “Easiest problems” that would be ok as well."
  },
  {
    "objectID": "hw/hw06.html",
    "href": "hw/hw06.html",
    "title": "Homework 6",
    "section": "",
    "text": "Due: Mar 1 by 11:59 pm eastern standard time\nSubmit a single file called name_06.py to Brightspace/OWL where name is replaced with your last name, e.g. gribble_06.py"
  },
  {
    "objectID": "hw/hw06.html#matplotlib-exercises",
    "href": "hw/hw06.html#matplotlib-exercises",
    "title": "Homework 6",
    "section": "Matplotlib exercises",
    "text": "Matplotlib exercises\n\nDefine a 1D array x containing 10 values starting at 0, ending at 0.9, in increments of 0.1. Define a vector y that is equal to np.sin(2 * np.pi * x). Generate a line plot with x on the horizontal axis and y on the vertical axis. Use blue circles at each data point (markersize of 8.0), connected by blue solid lines, with a linewidth of 2.0. Label the horizontal axis x and the vertical axis y. Set the range on the horizontal axis so it goes from 0 to 0.9 in steps of 0.1, and on the vertical axis so it goes from -1 to 1 in steps of 0.2.\n\n\n\n\n\n\n\n\n\nDefine 1D arrays y1 equal to [1,2,3,4,4], y2 equal to [1,5,6,8,10], and y3 equal to [5,4,2,1,1]. Generate a multi-line plot using squares (markersize 6.0) connected with solid lines (linewidth 1.0). Label the axes as shown, and set axis limits and axis ticks as shown. Add a legend as shown. The y1 colour is blue, the y2 colour is red, and the y3 colour is magenta.\n\n\n\n\n\n\n\n\n\nDefine a 100-length 1D array x starting at 1 and ending at 100 in increments of 1. Define y equal to (x * 0.15) + N where N is a 100 element 1D array of random values chosen from a gaussian distribution with mean 0.0 and standard deviation 0.5. Let z be equal to ((x * 0.05) + 2) + N2 where N2 is a 100 element 1D array of random values chosen from a gaussian distribution with mean 0.0 and standard deviation 2.0. Generate a scatterplot as shown below, using filled circles (markersize 3.0). The y colour is blue and the z colour is red. Pay attention to the axis labels, tick marks, and ranges. At the beginning of your answer set the random seed (once) to be equal to 9040 so that we all get the same random values, using np.random.seed(9040).\n\n\n\n\n\n\n\n\n\nRe-plot the data from Question 3 using subplots, as shown below. Try to replicate the axis limits, and axis labeling. Use boldface font to add a title to each subplot (A, B, and C, aligned to the left of each subplot as shown below).\nHint: adjust the hspace parameter for subplots so that the subplots don’t overlap, e.g. plt.subplots_adjust(hspace=0.6).\nHint: ax.spines[\"top\"].set_visible(False) will turn off the top part of the box outlining each plot.\nHint: fig = plt.figure() and then ax1 = fig.add_subplot(2,2,(1,2)) will generate a subplot that spans cells 1 and 2 (the top row) of the 2x2 grid. Then ax2 = fig.add_subplot(2,2,3) and ax3 = fig.add_subplot(2,2,4) will define the lower two sublots.\nHint: the legend() method of an axis has an option that will remove the frame outline: frameon=False\n\n\n\n\n\n\n\n\n\n\nDo your best to replicate all of the elements of the Figure. If you can’t replicate every little part of it don’t worry, but challenge yourself to try to get as close as you can. I will post a sample solution after the deadline."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scientific Computing FW25 (Jan-Apr 2026)",
    "section": "",
    "text": "URL\n\n\nwww.gribblelab.org/9040\n\n\n\n\nInstructor\n\n\nPaul Gribble (pgribble at uwo dot ca)\n\n\n\n\nClasses\n\n\nMondays & Thursdays, 9:30 am - 11:00 am in WIRB 1110\n\n\n\n\nTA\n\n\nAnthony Cruz (acruz27 at uwo dot ca)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Scientific Computing FW25 (Jan-Apr 2026)",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nDate\nTopic / Notes\nHomework\n\n\n\n\nJan 5/8\nIntro ; Setup ; uv ; Git\n-\n\n\nJan 12/15\nFundamentals (expressions & variables)\nHW01\n\n\nJan 19/22\nFundamentals (loops & conditionals)\nHW02\n\n\nJan 26/29\nFundamentals (data structures & functions)\nHW03\n\n\nFeb 2/5\nFundamentals (NumPy & File i/o)\nHW04\n\n\nFeb 9/12\nObject Oriented Programming (OOP)\nHW05\n\n\nFeb 16/19\nno classes (reading week)\n-\n\n\nFeb 23/26\nGraphical Displays of Data\nHW06\n\n\nMar 2/5\nReproducibility/Replicability I: plaintext authoring\nHW07\n\n\nMar 9/12\nno classes (Paul away) Reproducibility/Replicability II: clean code\n-\n\n\nMar 16/19\nSampling, Signal Processing, & Filtering Data\nHW08\n\n\nMar 23/26\nStatistical Thinking & Inferential Statistical Tests\nHW09\n\n\nMar 30/Apr 2\nFitting Models to Data\nHW10\n\n\nApr 6/Apr 9\nSimulating Dynamical Systems\n-\n\n\n\n\n\n\nconcepts covered each week\nsample code produced in class"
  },
  {
    "objectID": "index.html#readings",
    "href": "index.html#readings",
    "title": "Scientific Computing FW25 (Jan-Apr 2026)",
    "section": "Readings",
    "text": "Readings\n\nPython for Data Analysis by Wes McKinney\nThe Curious Coder’s Guide to Git by Matthew Brett\nBetter Code, Better Science by Russ Poldrack"
  },
  {
    "objectID": "oop.html",
    "href": "oop.html",
    "title": "Object Oriented Programming (OOP)",
    "section": "",
    "text": "So far in this course, we’ve been writing code as a sequence of instructions — define a variable, call a function, loop over a list. This style works great for small tasks, but as programs grow, it sometimes helps to organize code around objects: bundles of related data and behavior that model things in the world. These could be experimental participants, they could be datasets of specific types (EEG dataset, or fMRI dataset), they could be computational models (recurrent neural networks) or they could be statistical models (data + parameter estimates). It is not always better to use OOP but when it is, it’s often a lot better.\nObject-Oriented Programming (OOP) lets us define our own types (called classes) that combine:\n\nAttributes — the data an object holds (e.g., a circle’s radius)\nMethods — the functions an object can perform (e.g., calculating area)\n\nThink of a class as a blueprint and an object as a specific instance built from that blueprint. A class Circle describes what circles are; a particular circle with radius 5 is an instance of that class."
  },
  {
    "objectID": "oop.html#what-is-object-oriented-programming",
    "href": "oop.html#what-is-object-oriented-programming",
    "title": "Object Oriented Programming (OOP)",
    "section": "",
    "text": "So far in this course, we’ve been writing code as a sequence of instructions — define a variable, call a function, loop over a list. This style works great for small tasks, but as programs grow, it sometimes helps to organize code around objects: bundles of related data and behavior that model things in the world. These could be experimental participants, they could be datasets of specific types (EEG dataset, or fMRI dataset), they could be computational models (recurrent neural networks) or they could be statistical models (data + parameter estimates). It is not always better to use OOP but when it is, it’s often a lot better.\nObject-Oriented Programming (OOP) lets us define our own types (called classes) that combine:\n\nAttributes — the data an object holds (e.g., a circle’s radius)\nMethods — the functions an object can perform (e.g., calculating area)\n\nThink of a class as a blueprint and an object as a specific instance built from that blueprint. A class Circle describes what circles are; a particular circle with radius 5 is an instance of that class."
  },
  {
    "objectID": "oop.html#our-first-class-circle",
    "href": "oop.html#our-first-class-circle",
    "title": "Object Oriented Programming (OOP)",
    "section": "Our First Class: Circle",
    "text": "Our First Class: Circle\nimport math\n\nclass Circle:\n    \"\"\"A circle defined by its radius.\"\"\"\n\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\nLet’s unpack this piece by piece.\n\nThe __init__ Method (The Initializer)\n__init__ is a special method that Python calls automatically when you create a new object. Its job is to set up the object’s initial state by assigning attributes.\ndef __init__(self, radius):\n    self.radius = radius\n\nself refers to the specific object being created. Every method in a class receives self as its first argument — it’s how the object refers to itself.\nself.radius = radius stores the value you pass in as an attribute on the object.\n\nCreating an instance looks like a function call:\nc = Circle(5)\nprint(c.radius)  # 5\n\n\nAttributes\nAttributes are variables that belong to an object. You access them with dot notation:\nc = Circle(3)\nprint(c.radius)       # 3\nc.radius = 10         # you can change attributes too\nprint(c.radius)       # 10\n\n\nMethods\nMethods are functions defined inside a class. They always take self as the first parameter, which gives them access to the object’s attributes.\nc = Circle(5)\nprint(c.area())       # 78.539...\nprint(c.perimeter())  # 31.415...\nNotice that when you call a method, you don’t pass self — Python handles that for you."
  },
  {
    "objectID": "oop.html#built-in-special-methods",
    "href": "oop.html#built-in-special-methods",
    "title": "Object Oriented Programming (OOP)",
    "section": "Built-in Special Methods",
    "text": "Built-in Special Methods\nPython has a set of “magic” or “dunder” (double-underscore) methods that let your objects work with built-in Python features like print(), ==, &lt;, and more.\n\n__str__ — Human-Readable Display\n__str__ controls what print() shows:\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\n    def __str__(self):\n        return f\"Circle(radius={self.radius})\"\nc = Circle(5)\nprint(c)  # Circle(radius=5)\nWithout __str__, printing an object gives you something unhelpful like &lt;__main__.Circle object at 0x7f3b...&gt;.\n\n\n__repr__ — Developer-Friendly Display\n__repr__ is meant to give an unambiguous representation, often one you could paste back into Python to recreate the object. It’s what you see when you type a variable name in the interactive console:\ndef __repr__(self):\n    return f\"Circle({self.radius})\"\nc = Circle(5)\nc          # In a notebook or REPL, this shows: Circle(5)\nprint(c)   # This still uses __str__:       Circle(radius=5)\n\nRule of thumb: __str__ is for users, __repr__ is for developers. If you only define one, define __repr__ — Python will fall back to it when __str__ is missing."
  },
  {
    "objectID": "oop.html#comparison-operators",
    "href": "oop.html#comparison-operators",
    "title": "Object Oriented Programming (OOP)",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nWhat if we want to compare two circles? We can define what &lt;, &gt;, and == mean for our class. A natural choice is to compare by area.\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\n    def __str__(self):\n        return f\"Circle(radius={self.radius})\"\n\n    def __repr__(self):\n        return f\"Circle({self.radius})\"\n\n    def __eq__(self, other):\n        return self.area() == other.area()\n\n    def __lt__(self, other):\n        return self.area() &lt; other.area()\n\n    def __gt__(self, other):\n        return self.area() &gt; other.area()\nsmall = Circle(2)\nbig = Circle(7)\n\nprint(small == big)   # False\nprint(small &lt; big)    # True\nprint(small &gt; big)    # False\nA bonus: once you define __lt__, you can use sorted() on a list of circles!\ncircles = [Circle(5), Circle(1), Circle(3)]\nsorted_circles = sorted(circles)\nprint(sorted_circles)  # [Circle(1), Circle(3), Circle(5)]"
  },
  {
    "objectID": "oop.html#inheritance-building-on-existing-classes",
    "href": "oop.html#inheritance-building-on-existing-classes",
    "title": "Object Oriented Programming (OOP)",
    "section": "Inheritance: Building on Existing Classes",
    "text": "Inheritance: Building on Existing Classes\nInheritance lets you create a new class based on an existing one. The new class (the child or subclass) inherits all the attributes and methods of the original (the parent or superclass), and can add or change behavior.\nLet’s create a base class Shape and build specific shapes from it.\nclass Shape:\n    \"\"\"Base class for all shapes.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def area(self):\n        raise NotImplementedError(\"Subclasses must implement area()\")\n\n    def perimeter(self):\n        raise NotImplementedError(\"Subclasses must implement perimeter()\")\n\n    def __str__(self):\n        return f\"{self.name} with area {self.area():.2f}\"\n\n    def __repr__(self):\n        return f\"{self.name}()\"\n\n    def __eq__(self, other):\n        return self.area() == other.area()\n\n    def __lt__(self, other):\n        return self.area() &lt; other.area()\n\n    def __gt__(self, other):\n        return self.area() &gt; other.area()\nA few things to notice:\n\nShape is not meant to be used directly — it’s a template. Calling area() on a plain Shape raises an error on purpose, reminding us that each specific shape must provide its own version.\nThe comparison methods and __str__ are defined here once and inherited by all subclasses, so we don’t have to rewrite them.\n\n\nDefining Subclasses\nA subclass is defined by putting the parent class name in parentheses:\nclass Circle(Shape):\n    def __init__(self, name, radius):\n        super().__init__(name)  # call the parent's __init__\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\n    def __repr__(self):\n        return f\"Circle('{self.name}', radius={self.radius})\"\n\n\nclass Square(Shape):\n    def __init__(self, name, side):\n        super().__init__(name)\n        self.side = side\n\n    def area(self):\n        return self.side ** 2\n\n    def perimeter(self):\n        return 4 * self.side\n\n    def __repr__(self):\n        return f\"Square('{self.name}', side={self.side})\"\n\n\nclass Rectangle(Shape):\n    def __init__(self, name, width, height):\n        super().__init__(name)\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n\n    def __repr__(self):\n        return f\"Rectangle('{self.name}', width={self.width}, height={self.height})\"\n\n\nsuper() — Calling the Parent\nsuper().__init__(name) calls the Shape.__init__ method, which sets self.name. This way we reuse the parent’s setup logic instead of duplicating it. Now each shape can have a descriptive name: Circle(\"pizza\", 10.5) or Square(\"window\", 3).\n\n\nOverriding Methods\nWhen a subclass defines a method that already exists in the parent, the subclass version overrides it. Each shape above overrides area(), perimeter(), and __repr__() to provide its own behavior, while __str__, __eq__, __lt__, and __gt__ are inherited as-is from Shape."
  },
  {
    "objectID": "oop.html#polymorphism-same-interface-different-behavior",
    "href": "oop.html#polymorphism-same-interface-different-behavior",
    "title": "Object Oriented Programming (OOP)",
    "section": "Polymorphism: Same Interface, Different Behavior",
    "text": "Polymorphism: Same Interface, Different Behavior\nPolymorphism means “many forms.” Because every shape implements area() and perimeter(), we can write code that works with any shape without knowing which specific type it is:\nshapes = [Circle(\"pizza\", 5), Square(\"window\", 4), Rectangle(\"desk\", 3, 6)]\n\nfor shape in shapes:\n    print(f\"{shape.name}: area = {shape.area():.2f}, perimeter = {shape.perimeter():.2f}\")\nOutput:\npizza: area = 78.54, perimeter = 31.42\nwindow: area = 16.00, perimeter = 16.00\ndesk: area = 18.00, perimeter = 18.00\nThe loop doesn’t care whether it’s dealing with a circle, square, or rectangle. It just calls .area() and .perimeter(), and each object responds with its own version. That’s polymorphism.\nWe can also compare shapes of completely different types, because comparison is based on area:\nc = Circle(\"frisbee\", 3)       # area ≈ 28.27\ns = Square(\"coaster\", 6)       # area = 36.00\nr = Rectangle(\"postcard\", 5, 4) # area = 20.00\n\nprint(c &gt; r)         # True  — circle has more area than rectangle\nprint(s &gt; c)         # True  — square has more area than circle\n\n# Sort a mixed list of shapes by area\nall_shapes = [s, c, r]\nprint(sorted(all_shapes))\n# [Rectangle('postcard', width=5, height=4), Circle('frisbee', radius=3), Square('coaster', side=6)]"
  },
  {
    "objectID": "oop.html#putting-it-all-together",
    "href": "oop.html#putting-it-all-together",
    "title": "Object Oriented Programming (OOP)",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nHere’s the complete code in one block for reference:\nimport math\n\n\nclass Shape:\n    \"\"\"Base class for all shapes.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def area(self):\n        raise NotImplementedError(\"Subclasses must implement area()\")\n\n    def perimeter(self):\n        raise NotImplementedError(\"Subclasses must implement perimeter()\")\n\n    def __str__(self):\n        return f\"{self.name} with area {self.area():.2f}\"\n\n    def __repr__(self):\n        return f\"{self.name}()\"\n\n    def __eq__(self, other):\n        return self.area() == other.area()\n\n    def __lt__(self, other):\n        return self.area() &lt; other.area()\n\n    def __gt__(self, other):\n        return self.area() &gt; other.area()\n\n\nclass Circle(Shape):\n    def __init__(self, name, radius):\n        super().__init__(name)\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\n    def __repr__(self):\n        return f\"Circle('{self.name}', radius={self.radius})\"\n\n\nclass Square(Shape):\n    def __init__(self, name, side):\n        super().__init__(name)\n        self.side = side\n\n    def area(self):\n        return self.side ** 2\n\n    def perimeter(self):\n        return 4 * self.side\n\n    def __repr__(self):\n        return f\"Square('{self.name}', side={self.side})\"\n\n\nclass Rectangle(Shape):\n    def __init__(self, name, width, height):\n        super().__init__(name)\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n\n    def __repr__(self):\n        return f\"Rectangle('{self.name}', width={self.width}, height={self.height})\"\n\n\n# --- Demo ---\nshapes = [Circle(\"pizza\", 5), Square(\"window\", 4), Rectangle(\"desk\", 3, 6)]\n\nfor shape in shapes:\n    print(shape)\n\nprint()\nprint(\"Sorted by area:\")\nfor shape in sorted(shapes):\n    print(f\"{repr(shape):&gt;36s}  -&gt;  area = {shape.area():.2f}\")\nOutput:\npizza with area 78.54\nwindow with area 16.00\ndesk with area 18.00\n\nSorted by area:\n            Square('window', side=4)  -&gt;  area = 16.00\nRectangle('desk', width=3, height=6)  -&gt;  area = 18.00\n           Circle('pizza', radius=5)  -&gt;  area = 78.54"
  },
  {
    "objectID": "oop.html#quick-reference",
    "href": "oop.html#quick-reference",
    "title": "Object Oriented Programming (OOP)",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\nConcept\nWhat It Means\nWhere We Saw It\n\n\n\n\nClass\nA blueprint for creating objects\nclass Shape:\n\n\nInstance\nA specific object created from a class\nc = Circle(\"pizza\", 5)\n\n\nAttribute\nData stored on an object\nself.radius\n\n\nMethod\nA function that belongs to an object\ndef area(self):\n\n\n__init__\nSets up a new object’s initial state\ndef __init__(self, name, radius):\n\n\n__str__\nControls what print() displays\n\"pizza with area 78.54\"\n\n\n__repr__\nDeveloper-friendly representation\n\"Circle('pizza', radius=5)\"\n\n\n__eq__, __lt__, __gt__\nDefine ==, &lt;, &gt; for your objects\nComparing shapes by area\n\n\nInheritance\nA child class reuses a parent’s code\nclass Circle(Shape):\n\n\nsuper()\nCalls a method from the parent class\nsuper().__init__(name)\n\n\nOverriding\nA child replaces a parent’s method\nEach shape defines its own area()\n\n\nPolymorphism\nSame method name, different behavior per type\nLooping over mixed shapes"
  },
  {
    "objectID": "oop.html#resources-on-oop",
    "href": "oop.html#resources-on-oop",
    "title": "Object Oriented Programming (OOP)",
    "section": "Resources on OOP",
    "text": "Resources on OOP\n\nCS50P (Harvard online course)\n\nnotes\nvideo\n\nClasses and Objects — the Basics\nClasses and Objects — Digging a little deeper"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set Up Your Computer",
    "section": "",
    "text": "Yes.\nSort of.\nFor this course, we will be working extensively with the command line and VS Code, which function similarly across all three major operating system, so you can use whichever OS you prefer or already have. Linux is the gold standard in scientific computing environments; most computing clusters, cloud servers, and neuroimaging pipelines run on it, so familiarity with its command line is invaluable. MacOS runs on a Unix variant under the hood, meaning the terminal experience is nearly identical, making it a convenient choice for researchers who want a polished laptop environment that translates well to work on Linux/Unix environments such as servers and clusters.\nWindows has historically been the odd one out for scientific computing, but the Windows Subsystem for Linux (WSL) has changed this dramatically. The WSL lets you run a full Linux environment inside Windows, giving you access to the same tools and workflows. For our purposes, the key is having a working terminal and Python environment.\nSo if you are already using Unix/Linux, good news, you are all set, just some configuration things and some package installs and your computing environment will be ready for a modern course in coding / scientific computing.\nIf you are using MacOS, also good news, you may not know it but MacOS is based on a Unix variant, so with some small configuration things and some package installs, you will also be ready.\nIf you are using Windows, it will be ok. Something we will cover in this course is learning to use the terminal, learning about file systems, files, directories, and so on. On Linux/Unix and on MacOS (which is based on Unix) this is straightforward and you won’t need to do anything particularly special. On Windows however you will need to do some initial setup to install something called the Windows Subsystem for Linux (WSL). This will enable you to launch a Linux session within Windows, and follow along in the course with everyone else. It’s not such a big deal. Instructions are below.\nPS: Personally I am most familiar with MacOS and Unix/Linux. I am quite unfamiliar with Windows.\n\n\nOpen up an Ubuntu terminal and (assuming Ubuntu Linux) update the system:\nsudo apt update\nsudo apt upgrade -y\nand then install some necessary tools including git:\nsudo apt install -y build-essential git curl\nand then install the uv tool:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nclose your Terminal.\nand then download Visual Studio Code and install it.\n\n\n\nOpen up a terminal and install some necessary build tools:\nxcode-select --install\nInstall the Homebrew package manager:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThen close the terminal and open up a new one, and install some other necessary tools including git:\nbrew install git\nThen install the uv tool:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nclose your Terminal.\nand then download Visual Studio Code and install it.\nIf you want a nicer looking Terminal than the one that ships with MacOS you can try one of these:\n\niTerm2\nghostty\n\n\n\n\nFirst update Windows, I think you can access this by going to the Start menu and typing “update” and one of the options will be a system update.\nNext, open up Powershell (again, go to the Start menu and type Powershell to access it) and type:\nwsl --install\nWhen it is done, reboot your computer.\nOpen up Powershell again and type:\nwsl.exe --install Ubuntu\nIt will eventually ask you for a default Unix user account and password.\nClose Powershell and any other junky windows that Windows may have opened.\nOpen Powershell and check to see which version of WSL is running:\nwsl -l -v\nif it shows version 1 instead of version 2 then switch:\nwsl --set-version Ubuntu 2\nClose Powershell and reboot the computer.\nFrom your Start menu select “Ubuntu” and it should launch a terminal, which is now, running Ubuntu linux.\nNow go through the steps above in the Unix/Linux section."
  },
  {
    "objectID": "setup.html#macos-windows-or-linux",
    "href": "setup.html#macos-windows-or-linux",
    "title": "Set Up Your Computer",
    "section": "",
    "text": "Yes.\nSort of.\nFor this course, we will be working extensively with the command line and VS Code, which function similarly across all three major operating system, so you can use whichever OS you prefer or already have. Linux is the gold standard in scientific computing environments; most computing clusters, cloud servers, and neuroimaging pipelines run on it, so familiarity with its command line is invaluable. MacOS runs on a Unix variant under the hood, meaning the terminal experience is nearly identical, making it a convenient choice for researchers who want a polished laptop environment that translates well to work on Linux/Unix environments such as servers and clusters.\nWindows has historically been the odd one out for scientific computing, but the Windows Subsystem for Linux (WSL) has changed this dramatically. The WSL lets you run a full Linux environment inside Windows, giving you access to the same tools and workflows. For our purposes, the key is having a working terminal and Python environment.\nSo if you are already using Unix/Linux, good news, you are all set, just some configuration things and some package installs and your computing environment will be ready for a modern course in coding / scientific computing.\nIf you are using MacOS, also good news, you may not know it but MacOS is based on a Unix variant, so with some small configuration things and some package installs, you will also be ready.\nIf you are using Windows, it will be ok. Something we will cover in this course is learning to use the terminal, learning about file systems, files, directories, and so on. On Linux/Unix and on MacOS (which is based on Unix) this is straightforward and you won’t need to do anything particularly special. On Windows however you will need to do some initial setup to install something called the Windows Subsystem for Linux (WSL). This will enable you to launch a Linux session within Windows, and follow along in the course with everyone else. It’s not such a big deal. Instructions are below.\nPS: Personally I am most familiar with MacOS and Unix/Linux. I am quite unfamiliar with Windows.\n\n\nOpen up an Ubuntu terminal and (assuming Ubuntu Linux) update the system:\nsudo apt update\nsudo apt upgrade -y\nand then install some necessary tools including git:\nsudo apt install -y build-essential git curl\nand then install the uv tool:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nclose your Terminal.\nand then download Visual Studio Code and install it.\n\n\n\nOpen up a terminal and install some necessary build tools:\nxcode-select --install\nInstall the Homebrew package manager:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThen close the terminal and open up a new one, and install some other necessary tools including git:\nbrew install git\nThen install the uv tool:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nclose your Terminal.\nand then download Visual Studio Code and install it.\nIf you want a nicer looking Terminal than the one that ships with MacOS you can try one of these:\n\niTerm2\nghostty\n\n\n\n\nFirst update Windows, I think you can access this by going to the Start menu and typing “update” and one of the options will be a system update.\nNext, open up Powershell (again, go to the Start menu and type Powershell to access it) and type:\nwsl --install\nWhen it is done, reboot your computer.\nOpen up Powershell again and type:\nwsl.exe --install Ubuntu\nIt will eventually ask you for a default Unix user account and password.\nClose Powershell and any other junky windows that Windows may have opened.\nOpen Powershell and check to see which version of WSL is running:\nwsl -l -v\nif it shows version 1 instead of version 2 then switch:\nwsl --set-version Ubuntu 2\nClose Powershell and reboot the computer.\nFrom your Start menu select “Ubuntu” and it should launch a terminal, which is now, running Ubuntu linux.\nNow go through the steps above in the Unix/Linux section."
  },
  {
    "objectID": "setup.html#check-your-setup",
    "href": "setup.html#check-your-setup",
    "title": "Set Up Your Computer",
    "section": "Check your setup",
    "text": "Check your setup\nCreate a new folder somewhere on your computer’s file system and name it Psych_9040, and then navigate inside of it.\nOpen Ubuntu:\nmkdir Psych_9040\ncd Psych_9040\nUse uv to install a python environment\nuv init --python 3.12\nAdd the numpy package\nuv add numpy\nCreate a new empty file:\ntouch hello.py\nand add some Python lines of code to it:\necho \"import numpy as np\" &gt;&gt; hello.py\necho \"x = np.array ([2, 3, 5, 7, 11, 13, 17, 19, 23, 29])\" &gt;&gt; hello.py\necho \"y = np.sum(x)\" &gt;&gt; hello.py\necho \"print(f\\\"the sum of {x} is {y}\\\")\" &gt;&gt; hello.py\nand test the code:\nuv run python hello.py\nand you should see this output:\nthe sum of [ 2  3  5  7 11 13 17 19 23 29] is 129"
  },
  {
    "objectID": "setup.html#gitgithub",
    "href": "setup.html#gitgithub",
    "title": "Set Up Your Computer",
    "section": "Git/GitHub",
    "text": "Git/GitHub\nWe will be learning about and using code versioning with Git and GitHub, so you should sign up for a (free) GitHub account."
  },
  {
    "objectID": "setup.html#visual-studio-code",
    "href": "setup.html#visual-studio-code",
    "title": "Set Up Your Computer",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nAfter installing VS Code, launch it, and enable launching from the terminal.\n\nMacOS\n\nOpen VS Code: Launch the Visual Studio Code application.\nInstall Shell Command by Open the Command Palette by pressing Shift + Command + P (or F1).\nType “shell command” into the prompt.\nSelect the command “Shell Command: Install ‘code’ command in PATH”. (You might be prompted for your administrator password.)\nLaunch from Terminal: Open your terminal. Navigate to the directory of your project. Type the following command:\n\ncode .\nThe . opens the current directory in a new VS Code window. You can replace . with a specific file name or directory path.\n\n\nWSL on Windows\n\nLaunch VS Code in your Windows environment.\nOpen the Command Palette (Ctrl + Shift + P).\nType “shell command” and select “Shell Command: Install ‘code’ command in PATH” if you didn’t do so during the initial installation of the Remote - WSL extension. The extension should typically handle this installation, but verifying doesn’t hurt.\nLaunch from WSL Terminal: Open your WSL terminal (e.g., Ubuntu). Navigate to your project directory within the Linux environment. Type the command:\n\ncode .\nThis command will open the current Linux directory in a new VS Code window on your Windows desktop, automatically connecting via the Remote - WSL extension."
  }
]